{"cells":[{"cell_type":"markdown","metadata":{"id":"VYqASlKXhSf4"},"source":["# Basic Instructions\n","\n","1. Enter your Name, UID and Link to Google Drive in the provided space.\n","2. Submit the assignment to Gradescope.\n","\n","\n","Final Submission Deadline: May 2, 5:00pm\n","\n","Late Submission Deadline: May 4, 5:00pm"]},{"cell_type":"markdown","metadata":{"id":"m3d1a2uihZGA"},"source":["Name:  **Sumedh Koppula**  \n","UID:  **117386066**\n","\n","Link to Google Drive : **https://drive.google.com/file/d/1ta6aqyCjJe2LsPpuibmVFRHWo5prbSlA/view?usp=sharing**"]},{"cell_type":"markdown","metadata":{"id":"jk4V_YE-hZbX"},"source":["In this assignment, you will learn how to use transformers to generate text. Specifically, you will implement very small GPT model. It will predict streams of characters to attempt to form nice sounding sentences.\n","\n","You will complete 5 exercises, described in detail later on in this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T21:11:14.083915Z","iopub.status.busy":"2023-04-28T21:11:14.083312Z","iopub.status.idle":"2023-04-28T21:11:14.090707Z","shell.execute_reply":"2023-04-28T21:11:14.089641Z","shell.execute_reply.started":"2023-04-28T21:11:14.083869Z"},"trusted":true,"id":"Sc6IYcbufL5S","outputId":"22e84631-4604-4f48-8276-73cb1e1d6559"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pytorch version：\n","2.0.0+cu117\n","CUDA Version: \n","11.7\n","cuDNN version is :\n","8500\n"]}],"source":["import torch\n","print(\"Pytorch version：\")\n","print(torch.__version__)\n","print(\"CUDA Version: \")\n","print(torch.version.cuda)\n","print(\"cuDNN version is :\")\n","print(torch.backends.cudnn.version())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:20.205494Z","iopub.status.busy":"2023-04-28T19:54:20.204772Z","iopub.status.idle":"2023-04-28T19:54:20.211336Z","shell.execute_reply":"2023-04-28T19:54:20.210267Z","shell.execute_reply.started":"2023-04-28T19:54:20.205453Z"},"id":"EXspyciwR-8z","trusted":true},"outputs":[],"source":["import os\n","import time\n","import math\n","import pickle\n","from contextlib import nullcontext\n","\n","import numpy as np\n","import torch\n","from torch.distributed import init_process_group, destroy_process_group\n","\n","import os\n","import pickle\n","import requests\n","import numpy as np\n","\n","import math\n","import inspect\n","from dataclasses import dataclass\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{"id":"XKLs5jsr-uSv"},"source":["## DATA PREPARATION "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-28T19:54:08.497347Z","iopub.status.busy":"2023-04-28T19:54:08.494498Z","iopub.status.idle":"2023-04-28T19:54:09.256921Z","shell.execute_reply":"2023-04-28T19:54:09.255371Z","shell.execute_reply.started":"2023-04-28T19:54:08.497297Z"},"id":"h0SfnEVkiV04","outputId":"624249f1-9b01-41b5-d0bb-2fdedd4a04f0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dataset in characters: 1,115,395\n","all the unique characters: \n"," !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","vocab size: 65\n","train has 1,003,855 tokens\n","val has 111,540 tokens\n"]}],"source":["# download the tiny shakespeare dataset\n","if not os.path.exists('data'):\n","  os.makedirs('data')\n","if not os.path.exists('data/shakespeare'):\n","  os.makedirs('data/shakespeare')\n","data_root = 'data/shakespeare'\n","input_file_path = os.path.join(data_root, 'input.txt')\n","if not os.path.exists(input_file_path):\n","    data_url = 'https://raw.githubusercontent.com/learn2phoenix/CMSC472_HW6/main/input.txt'\n","    with open(input_file_path, 'w') as f:\n","        f.write(requests.get(data_url).text)\n","\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","print(f\"length of dataset in characters: {len(data):,}\")\n","\n","# get all the unique characters that occur in this text\n","chars = sorted(list(set(data)))\n","vocab_size = len(chars)\n","print(\"all the unique characters:\", ''.join(chars))\n","print(f\"vocab size: {vocab_size:,}\")\n","\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","def encode(s):\n","    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","def decode(l):\n","    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# create the train and test splits\n","n = len(data)\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]\n","\n","# encode both to integers\n","train_ids = encode(train_data)\n","val_ids = encode(val_data)\n","print(f\"train has {len(train_ids):,} tokens\")\n","print(f\"val has {len(val_ids):,} tokens\")\n","\n","# export to bin files\n","train_ids = np.array(train_ids, dtype=np.uint16)\n","val_ids = np.array(val_ids, dtype=np.uint16)\n","train_ids.tofile(os.path.join(data_root, 'train.bin'))\n","val_ids.tofile(os.path.join(data_root, 'val.bin'))\n","\n","# save the meta information as well, to help us encode/decode later\n","meta = {\n","    'vocab_size': vocab_size,\n","    'itos': itos,\n","    'stoi': stoi,\n","}\n","with open(f'{data_root}/meta.pkl', 'wb') as f:\n","    pickle.dump(meta, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"execution":{"iopub.execute_input":"2023-04-28T19:54:09.259579Z","iopub.status.busy":"2023-04-28T19:54:09.259274Z","iopub.status.idle":"2023-04-28T19:54:09.271938Z","shell.execute_reply":"2023-04-28T19:54:09.269860Z","shell.execute_reply.started":"2023-04-28T19:54:09.259551Z"},"id":"zfW-HWLs0FxX","outputId":"5fdbdab2-c1be-4492-bd41-3fbf7f995033","trusted":true},"outputs":[{"data":{"text/plain":["'data/shakespeare/input.txt'"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["input_file_path"]},{"cell_type":"markdown","metadata":{"id":"kg2abLjoAyrl"},"source":["##Complete the TODO sections in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:09.279473Z","iopub.status.busy":"2023-04-28T19:54:09.277446Z","iopub.status.idle":"2023-04-28T19:54:09.288602Z","shell.execute_reply":"2023-04-28T19:54:09.287480Z","shell.execute_reply.started":"2023-04-28T19:54:09.279444Z"},"id":"5CrWG-UEWXRg","trusted":true},"outputs":[],"source":["# @torch.jit.script # good to enable when not using torch.compile, disable when using (our default)\n","def new_gelu(x):\n","    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","\n","class LayerNorm(nn.Module):\n","\n","    def __init__(self, ndim, bias):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"]},{"cell_type":"markdown","metadata":{"id":"NqK0wyxEggcD"},"source":["###Exercise 1\n","Complete the forward function for `CausalSelfAttention` class. Most of the function is already implemented, you just have to compute the query, key and values."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:09.293318Z","iopub.status.busy":"2023-04-28T19:54:09.292874Z","iopub.status.idle":"2023-04-28T19:54:09.315384Z","shell.execute_reply":"2023-04-28T19:54:09.314199Z","shell.execute_reply.started":"2023-04-28T19:54:09.293274Z"},"id":"SxQvYKPQWZSo","trusted":true},"outputs":[],"source":["class CausalSelfAttention(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embd % config.n_head == 0\n","        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n","        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n","        self.attn_dropout = nn.Dropout(config.dropout)\n","        self.resid_dropout = nn.Dropout(config.dropout)\n","        self.n_head = config.n_head\n","        self.n_embd = config.n_embd\n","        self.dropout = config.dropout\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n","            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n","                                        .view(1, 1, config.block_size, config.block_size))\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","\n","        # TODO: you should calculate key, query, values (k, q, v) from `x` for all heads in batch.\n","        # Don't forget to move head forward to be the batch dim\n","        # HINT: using self.c_attn and splits to have q, k, v\n","        # YOUR CODE BEGINS HERE\n","        qkv = self.c_attn(x)\n","        q, k, v = torch.chunk(qkv, 3, dim=-1)\n","        q = q.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n","        k = k.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n","        v = v.view(B, T, self.n_head, self.n_embd // self.n_head).transpose(1, 2)\n","        # YOUR CODE ENDS HERE\n","        if self.flash:\n","            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n","        else:\n","            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n","            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            att = self.attn_dropout(att)\n","            y = att @ v\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"uRoKHgUHkJa-"},"source":["Some other utility blocks are defined as:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:09.318147Z","iopub.status.busy":"2023-04-28T19:54:09.317501Z","iopub.status.idle":"2023-04-28T19:54:09.333357Z","shell.execute_reply":"2023-04-28T19:54:09.332038Z","shell.execute_reply.started":"2023-04-28T19:54:09.318109Z"},"id":"4CKrJM5MWdyA","trusted":true},"outputs":[],"source":["class MLP(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n","        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n","        self.dropout = nn.Dropout(config.dropout)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = new_gelu(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n","        self.mlp = MLP(config)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.mlp(self.ln_2(x))\n","        return x\n","\n","@dataclass\n","class GPTConfig:\n","    block_size: int = 1024\n","    vocab_size: int = 50304\n","    n_layer: int = 12\n","    n_head: int = 12\n","    n_embd: int = 768\n","    dropout: float = 0.0\n","    bias: bool = True"]},{"cell_type":"markdown","metadata":{"id":"Zh1NjhlDgrLh"},"source":["### Exercise 2\n","Complete the forward function for `GPT` class. Most of the function is again already implemented, you need to do forward for `self.transformer` of this class. \n","\n","**HINT:** Read the token and position embeddings, forward through each block in loop and then forward through last layer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:09.336562Z","iopub.status.busy":"2023-04-28T19:54:09.335693Z","iopub.status.idle":"2023-04-28T19:54:09.386495Z","shell.execute_reply":"2023-04-28T19:54:09.385213Z","shell.execute_reply.started":"2023-04-28T19:54:09.336528Z"},"id":"x0nmLFiFnKLN","trusted":true},"outputs":[],"source":["class GPT(nn.Module):\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.vocab_size is not None\n","        assert config.block_size is not None\n","        self.config = config\n","\n","        self.transformer = nn.ModuleDict(dict(\n","            wte = nn.Embedding(config.vocab_size, config.n_embd),\n","            wpe = nn.Embedding(config.block_size, config.n_embd),\n","            drop = nn.Dropout(config.dropout),\n","            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n","            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n","        ))\n","        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n","        self.transformer.wte.weight = self.lm_head.weight\n","\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n","\n","        # report number of parameters\n","        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n","\n","    def get_num_params(self, non_embedding=True):\n","        \"\"\"\n","        Return the number of parameters in the model.\n","        remember to subtract the position embeddings for non_embedding\n","        The token embeddings would have received the same treatement too, but \n","        for their use as weights, due to parameter sharing, in the final layer.\n","        \"\"\"\n","        n_params = sum(p.numel() for p in self.parameters())\n","        if non_embedding:\n","            n_params -= self.transformer.wpe.weight.numel()\n","        return n_params\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n","        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n","\n","        # TODO: write the forward for the GPT model and assign output to x. HINT: Refer to definition for self.transformer\n","        # YOUR CODE BEGINS HERE\n","        x = self.transformer[\"wte\"](idx) + self.transformer[\"wpe\"](pos)\n","        x = self.transformer[\"drop\"](x)\n","\n","        for block in self.transformer[\"h\"]:\n","            x = block(x)\n","\n","        x = self.transformer[\"ln_f\"](x)\n","        # YOUR CODE ENDS HERE\n","\n","        if targets is not None:\n","            logits = self.lm_head(x)\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n","        else:\n","            logits = self.lm_head(x[:, [-1], :])\n","            loss = None\n","\n","        return logits, loss\n","\n","    def crop_block_size(self, block_size):\n","        assert block_size <= self.config.block_size\n","        self.config.block_size = block_size\n","        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n","        for block in self.transformer.h:\n","            if hasattr(block.attn, 'bias'):\n","                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n","\n","    @classmethod\n","    def from_pretrained(cls, model_type, override_args=None):\n","        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n","        override_args = override_args or {}\n","        assert all(k == 'dropout' for k in override_args)\n","        from transformers import GPT2LMHeadModel\n","        print(\"loading weights from pretrained gpt: %s\" % model_type)\n","\n","\n","        config_args = {\n","            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n","            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n","            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n","            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n","        }[model_type]\n","        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n","        config_args['vocab_size'] = 50257\n","        config_args['block_size'] = 1024\n","        config_args['bias'] = True\n","        if 'dropout' in override_args:\n","            print(f\"overriding dropout rate to {override_args['dropout']}\")\n","            config_args['dropout'] = override_args['dropout']\n","        config = GPTConfig(**config_args)\n","        model = GPT(config)\n","        sd = model.state_dict()\n","        sd_keys = sd.keys()\n","        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] \n","        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n","        sd_hf = model_hf.state_dict()\n","\n","        sd_keys_hf = sd_hf.keys()\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n","        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n","        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n","        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n","        for k in sd_keys_hf:\n","            if any(k.endswith(w) for w in transposed):\n","                assert sd_hf[k].shape[::-1] == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k].t())\n","            else:\n","                assert sd_hf[k].shape == sd[k].shape\n","                with torch.no_grad():\n","                    sd[k].copy_(sd_hf[k])\n","\n","        return model\n","\n","    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n","        decay = set()\n","        no_decay = set()\n","        whitelist_weight_modules = (torch.nn.Linear, )\n","        blacklist_weight_modules = (torch.nn.LayerNorm, LayerNorm, torch.nn.Embedding)\n","        for mn, m in self.named_modules():\n","            for pn, p in m.named_parameters():\n","                fpn = '%s.%s' % (mn, pn) if mn else pn\n","                if pn.endswith('bias'):\n","                    no_decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n","                    decay.add(fpn)\n","                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n","                    no_decay.add(fpn)\n","\n","        decay.remove('lm_head.weight')\n","\n","        param_dict = {pn: p for pn, p in self.named_parameters()}\n","        inter_params = decay & no_decay\n","        union_params = decay | no_decay\n","        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n","        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n","                                                    % (str(param_dict.keys() - union_params), )\n","\n","        optim_groups = [\n","            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n","            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n","        ]\n","        use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n","        print(f\"using fused AdamW: {use_fused}\")\n","        extra_args = dict(fused=True) if use_fused else dict()\n","        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n","\n","        return optimizer\n","\n","    def estimate_mfu(self, fwdbwd_per_iter, dt):\n","        N = self.get_num_params()\n","        cfg = self.config\n","        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n","        flops_per_token = 6*N + 12*L*H*Q*T\n","        flops_per_fwdbwd = flops_per_token * T\n","        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n","        flops_achieved = flops_per_iter * (1.0/dt)\n","        flops_promised = 312e12 \n","        mfu = flops_achieved / flops_promised\n","        return mfu\n","\n","    @torch.no_grad()\n","    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            # pluck the logits at the final step and scale by desired temperature\n","            logits = logits[:, -1, :] / temperature\n","            # optionally crop the logits to only the top k options\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = -float('Inf')\n","            # apply softmax to convert logits to (normalized) probabilities\n","            probs = F.softmax(logits, dim=-1)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","        return idx"]},{"cell_type":"markdown","metadata":{"id":"FC8pX51nmXad"},"source":["## TRAINING"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-28T19:54:09.388533Z","iopub.status.busy":"2023-04-28T19:54:09.388143Z","iopub.status.idle":"2023-04-28T19:54:09.399930Z","shell.execute_reply":"2023-04-28T19:54:09.398628Z","shell.execute_reply.started":"2023-04-28T19:54:09.388499Z"},"id":"jZoStRV_mKNu","outputId":"eb61e626-6164-4473-aab9-f8066d1168d6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tokens per iteration will be: 16,384\n"]}],"source":["## TRAIN CONFIG\n","out_dir = 'out-shakespeare-char'\n","out_dir_email = 'out-enron-email'\n","eval_interval = 250\n","log_interval = 10\n","eval_iters = 200\n","eval_only = False\n","always_save_checkpoint = False\n","# data\n","dataset = 'shakespeare'\n","dataset_email = 'enron_data'\n","gradient_accumulation_steps = 1\n","batch_size = 64\n","block_size = 256\n","# model\n","n_layer = 6\n","n_head = 6\n","n_embd = 384\n","dropout = 0.2\n","bias =  False\n","# adamw optimizer\n","learning_rate = 1e-3\n","max_iters = 5000\n","weight_decay = 1e-1\n","beta1 = 0.9\n","beta2 = 0.99\n","grad_clip = 1.0\n","decay_lr = True\n","warmup_iters = 100\n","lr_decay_iters = 5000\n","min_lr = 1e-4\n","# system\n","device = 'cuda'\n","dtype = 'float16'\n","compile = False\n","\n","seed_offset = 0\n","ddp_world_size = 1\n","tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n","print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"]},{"cell_type":"markdown","metadata":{"id":"QonpBseImY7U"},"source":["### Exercise 3 \n","1. Complete the TODO sections in the cell below and train the model on the shakespeare data. Complete the `get_lr` function. You should implement your learning rate schedule here. Your learning rate schedule should involve linear warmup and cosine decay.\n","\n","Train the model. For training, you should get loss below 2.0 in roughly 10 minutes. You should not need to run for any longer than 20 minutes (on colab GPU) to get nice results. If you're just testing things out, consider training for only a minute or so at a time, and just confirming that loss decreases. You should only need to train from start to finish 1 time- when you're ready to submit."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-28T19:54:09.403380Z","iopub.status.busy":"2023-04-28T19:54:09.402307Z","iopub.status.idle":"2023-04-28T19:54:09.435137Z","shell.execute_reply":"2023-04-28T19:54:09.432249Z","shell.execute_reply.started":"2023-04-28T19:54:09.403343Z"},"id":"UQb2FyyaR68U","trusted":true},"outputs":[],"source":["def get_lr(it):\n","    if it > lr_decay_iters:\n","        return min_lr\n","    # TODO: Implement the learning rate schedule and return lr for the iteration\n","    # 1: include linear warmup\n","    # 2: implement cosine decay for after warmup (use warmup_iters from your hyperparams)\n","    # YOUR CODE BEGINS HERE\n","    if it < warmup_iters:\n","        decay = it / warmup_iters\n","    else:\n","        decay = 0.5 * (1 + math.cos(math.pi * (it - warmup_iters) / (lr_decay_iters - warmup_iters)))\n","    assert 0 <= decay <= 1\n","    coefficient = decay\n","    # YOUR CODE ENDS HERE\n","    return min_lr + coefficient * (learning_rate - min_lr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-28T19:54:09.437961Z","iopub.status.idle":"2023-04-28T19:54:09.440308Z","shell.execute_reply":"2023-04-28T19:54:09.440045Z","shell.execute_reply.started":"2023-04-28T19:54:09.440023Z"},"id":"EnoNsDhOiMi9","trusted":true,"outputId":"a83c0ade-10a4-451c-b484-e5f036ae7675"},"outputs":[{"name":"stdout","output_type":"stream","text":["found vocab_size = 65 (inside data/shakespeare/meta.pkl)\n","number of parameters: 10.65M\n","using fused AdamW: True\n","step 0: train loss 4.2875, val loss 4.2826\n","iter 0: loss 4.2559, time 26404.08ms, mfu -100.00%\n","iter 10: loss 3.1613, time 181.93ms, mfu 2.05%\n","iter 20: loss 2.7856, time 181.93ms, mfu 2.05%\n","iter 30: loss 2.6532, time 183.13ms, mfu 2.05%\n","iter 40: loss 2.5717, time 181.93ms, mfu 2.05%\n","iter 50: loss 2.5366, time 182.99ms, mfu 2.05%\n","iter 60: loss 2.5172, time 181.94ms, mfu 2.05%\n","iter 70: loss 2.5206, time 182.29ms, mfu 2.05%\n","iter 80: loss 2.4905, time 182.11ms, mfu 2.05%\n","iter 90: loss 2.4793, time 181.78ms, mfu 2.05%\n","iter 100: loss 2.4800, time 181.94ms, mfu 2.05%\n","iter 110: loss 2.4873, time 182.17ms, mfu 2.05%\n","iter 120: loss 2.4659, time 183.49ms, mfu 2.04%\n","iter 130: loss 2.4458, time 182.22ms, mfu 2.04%\n","iter 140: loss 2.4543, time 183.19ms, mfu 2.04%\n","iter 150: loss 2.4504, time 183.49ms, mfu 2.04%\n","iter 160: loss 2.4567, time 182.00ms, mfu 2.04%\n","iter 170: loss 2.4535, time 182.29ms, mfu 2.04%\n","iter 180: loss 2.4411, time 182.10ms, mfu 2.04%\n","iter 190: loss 2.4347, time 182.19ms, mfu 2.04%\n","iter 200: loss 2.4271, time 182.21ms, mfu 2.04%\n","iter 210: loss 2.4088, time 183.85ms, mfu 2.04%\n","iter 220: loss 2.4106, time 183.82ms, mfu 2.04%\n","iter 230: loss 2.4307, time 182.90ms, mfu 2.04%\n","iter 240: loss 2.4107, time 182.24ms, mfu 2.04%\n","step 250: train loss 2.3809, val loss 2.4105\n","saving checkpoint to out-shakespeare-char\n","iter 250: loss 2.4024, time 26510.42ms, mfu 1.84%\n","iter 260: loss 2.3928, time 182.18ms, mfu 1.86%\n","iter 270: loss 2.4040, time 182.10ms, mfu 1.88%\n","iter 280: loss 2.4118, time 182.69ms, mfu 1.89%\n","iter 290: loss 2.3968, time 183.49ms, mfu 1.91%\n","iter 300: loss 2.3730, time 181.90ms, mfu 1.92%\n","iter 310: loss 2.3788, time 181.87ms, mfu 1.93%\n","iter 320: loss 2.3640, time 182.26ms, mfu 1.95%\n","iter 330: loss 2.3529, time 183.32ms, mfu 1.95%\n","iter 340: loss 2.3663, time 182.34ms, mfu 1.96%\n","iter 350: loss 2.3519, time 183.23ms, mfu 1.97%\n","iter 360: loss 2.3305, time 182.77ms, mfu 1.98%\n","iter 370: loss 2.3331, time 182.44ms, mfu 1.98%\n","iter 380: loss 2.3078, time 181.77ms, mfu 1.99%\n","iter 390: loss 2.3258, time 183.06ms, mfu 1.99%\n","iter 400: loss 2.3274, time 181.73ms, mfu 2.00%\n","iter 410: loss 2.2860, time 182.30ms, mfu 2.00%\n","iter 420: loss 2.3032, time 183.02ms, mfu 2.01%\n","iter 430: loss 2.2584, time 183.55ms, mfu 2.01%\n","iter 440: loss 2.2375, time 182.25ms, mfu 2.01%\n","iter 450: loss 2.2449, time 182.80ms, mfu 2.02%\n","iter 460: loss 2.1925, time 182.86ms, mfu 2.02%\n","iter 470: loss 2.2173, time 182.41ms, mfu 2.02%\n","iter 480: loss 2.2067, time 185.87ms, mfu 2.02%\n","iter 490: loss 2.1668, time 181.50ms, mfu 2.02%\n","step 500: train loss 2.0697, val loss 2.1519\n","saving checkpoint to out-shakespeare-char\n","iter 500: loss 2.1579, time 26503.83ms, mfu 1.82%\n","iter 510: loss 2.1412, time 183.10ms, mfu 1.84%\n","iter 520: loss 2.1273, time 182.06ms, mfu 1.86%\n","iter 530: loss 2.1286, time 183.06ms, mfu 1.88%\n","iter 540: loss 2.1408, time 182.17ms, mfu 1.90%\n","iter 550: loss 2.0856, time 183.21ms, mfu 1.91%\n","iter 560: loss 2.1191, time 182.26ms, mfu 1.92%\n","iter 570: loss 2.1009, time 181.74ms, mfu 1.94%\n","iter 580: loss 2.0688, time 181.87ms, mfu 1.95%\n","iter 590: loss 2.0423, time 182.14ms, mfu 1.96%\n","iter 600: loss 2.0382, time 181.81ms, mfu 1.97%\n","iter 610: loss 2.0241, time 182.53ms, mfu 1.97%\n","iter 620: loss 2.0157, time 182.39ms, mfu 1.98%\n","iter 630: loss 2.0091, time 182.08ms, mfu 1.99%\n","iter 640: loss 1.9844, time 182.19ms, mfu 1.99%\n","iter 650: loss 1.9958, time 183.23ms, mfu 2.00%\n","iter 660: loss 1.9854, time 183.49ms, mfu 2.00%\n","iter 670: loss 1.9328, time 182.19ms, mfu 2.01%\n","iter 680: loss 1.9534, time 182.34ms, mfu 2.01%\n","iter 690: loss 1.9321, time 181.80ms, mfu 2.01%\n","iter 700: loss 1.9100, time 182.90ms, mfu 2.02%\n","iter 710: loss 1.9026, time 182.24ms, mfu 2.02%\n","iter 720: loss 1.8703, time 182.52ms, mfu 2.02%\n","iter 730: loss 1.9007, time 182.00ms, mfu 2.02%\n","iter 740: loss 1.8711, time 182.15ms, mfu 2.03%\n","step 750: train loss 1.7652, val loss 1.9186\n","saving checkpoint to out-shakespeare-char\n","iter 750: loss 1.8595, time 26604.04ms, mfu 1.82%\n","iter 760: loss 1.8759, time 183.85ms, mfu 1.84%\n","iter 770: loss 1.8488, time 184.75ms, mfu 1.86%\n","iter 780: loss 1.8343, time 182.77ms, mfu 1.88%\n","iter 790: loss 1.8415, time 183.74ms, mfu 1.89%\n","iter 800: loss 1.8403, time 183.65ms, mfu 1.91%\n","iter 810: loss 1.8143, time 182.61ms, mfu 1.92%\n","iter 820: loss 1.8196, time 183.75ms, mfu 1.93%\n","iter 830: loss 1.7727, time 183.76ms, mfu 1.94%\n","iter 840: loss 1.7906, time 183.23ms, mfu 1.95%\n","iter 850: loss 1.7670, time 183.90ms, mfu 1.96%\n","iter 860: loss 1.7428, time 182.49ms, mfu 1.97%\n","iter 870: loss 1.7772, time 183.70ms, mfu 1.97%\n","iter 880: loss 1.7481, time 182.53ms, mfu 1.98%\n","iter 890: loss 1.7215, time 182.53ms, mfu 1.99%\n","iter 900: loss 1.7372, time 184.97ms, mfu 1.99%\n","iter 910: loss 1.7552, time 184.62ms, mfu 1.99%\n","iter 920: loss 1.7382, time 182.51ms, mfu 2.00%\n","iter 930: loss 1.7348, time 183.16ms, mfu 2.00%\n","iter 940: loss 1.7136, time 183.62ms, mfu 2.00%\n","iter 950: loss 1.7114, time 182.96ms, mfu 2.01%\n","iter 960: loss 1.6945, time 184.14ms, mfu 2.01%\n","iter 970: loss 1.6825, time 183.97ms, mfu 2.01%\n","iter 980: loss 1.6777, time 182.36ms, mfu 2.01%\n","iter 990: loss 1.6683, time 182.54ms, mfu 2.02%\n","step 1000: train loss 1.5831, val loss 1.7665\n","saving checkpoint to out-shakespeare-char\n","iter 1000: loss 1.6923, time 26595.71ms, mfu 1.82%\n","iter 1010: loss 1.6807, time 183.42ms, mfu 1.84%\n","iter 1020: loss 1.6278, time 182.65ms, mfu 1.86%\n","iter 1030: loss 1.6606, time 182.83ms, mfu 1.88%\n","iter 1040: loss 1.6386, time 182.38ms, mfu 1.89%\n","iter 1050: loss 1.6706, time 183.29ms, mfu 1.91%\n","iter 1060: loss 1.6305, time 183.02ms, mfu 1.92%\n","iter 1070: loss 1.6478, time 183.92ms, mfu 1.93%\n","iter 1080: loss 1.6709, time 182.63ms, mfu 1.94%\n","iter 1090: loss 1.6688, time 187.71ms, mfu 1.95%\n","iter 1100: loss 1.5898, time 183.57ms, mfu 1.95%\n","iter 1110: loss 1.6179, time 182.70ms, mfu 1.96%\n","iter 1120: loss 1.5894, time 182.63ms, mfu 1.97%\n","iter 1130: loss 1.6350, time 183.24ms, mfu 1.98%\n","iter 1140: loss 1.5946, time 183.53ms, mfu 1.98%\n","iter 1150: loss 1.6044, time 183.21ms, mfu 1.99%\n","iter 1160: loss 1.6063, time 183.18ms, mfu 1.99%\n","iter 1170: loss 1.5884, time 184.36ms, mfu 1.99%\n","iter 1180: loss 1.6010, time 184.60ms, mfu 2.00%\n","iter 1190: loss 1.5642, time 183.53ms, mfu 2.00%\n","iter 1200: loss 1.5921, time 183.62ms, mfu 2.00%\n","iter 1210: loss 1.5891, time 182.79ms, mfu 2.01%\n","iter 1220: loss 1.5868, time 182.86ms, mfu 2.01%\n","iter 1230: loss 1.5773, time 182.68ms, mfu 2.01%\n","iter 1240: loss 1.5728, time 183.87ms, mfu 2.01%\n","step 1250: train loss 1.4759, val loss 1.6771\n","saving checkpoint to out-shakespeare-char\n","iter 1250: loss 1.5635, time 26544.92ms, mfu 1.81%\n","iter 1260: loss 1.5790, time 183.10ms, mfu 1.84%\n","iter 1270: loss 1.5611, time 183.99ms, mfu 1.86%\n","iter 1280: loss 1.5537, time 182.64ms, mfu 1.87%\n","iter 1290: loss 1.5667, time 182.82ms, mfu 1.89%\n","iter 1300: loss 1.5500, time 183.18ms, mfu 1.90%\n","iter 1310: loss 1.5643, time 182.92ms, mfu 1.92%\n","iter 1320: loss 1.5938, time 182.94ms, mfu 1.93%\n","iter 1330: loss 1.5327, time 182.89ms, mfu 1.94%\n","iter 1340: loss 1.5875, time 183.56ms, mfu 1.95%\n","iter 1350: loss 1.5447, time 183.46ms, mfu 1.96%\n","iter 1360: loss 1.5128, time 182.52ms, mfu 1.97%\n","iter 1370: loss 1.5373, time 182.51ms, mfu 1.97%\n","iter 1380: loss 1.5033, time 183.76ms, mfu 1.98%\n","iter 1390: loss 1.5306, time 184.11ms, mfu 1.98%\n","iter 1400: loss 1.5038, time 182.91ms, mfu 1.99%\n","iter 1410: loss 1.5042, time 182.51ms, mfu 1.99%\n","iter 1420: loss 1.5054, time 183.59ms, mfu 2.00%\n","iter 1430: loss 1.5295, time 184.11ms, mfu 2.00%\n","iter 1440: loss 1.4742, time 182.85ms, mfu 2.00%\n","iter 1450: loss 1.5133, time 182.71ms, mfu 2.01%\n","iter 1460: loss 1.4958, time 183.32ms, mfu 2.01%\n","iter 1470: loss 1.5022, time 182.74ms, mfu 2.01%\n","iter 1480: loss 1.4835, time 182.60ms, mfu 2.02%\n","iter 1490: loss 1.5221, time 182.88ms, mfu 2.02%\n","step 1500: train loss 1.4006, val loss 1.6183\n","saving checkpoint to out-shakespeare-char\n","iter 1500: loss 1.5172, time 26527.16ms, mfu 1.82%\n","iter 1510: loss 1.4883, time 184.32ms, mfu 1.84%\n","iter 1520: loss 1.4731, time 182.91ms, mfu 1.86%\n","iter 1530: loss 1.4916, time 183.59ms, mfu 1.88%\n","iter 1540: loss 1.5160, time 183.13ms, mfu 1.89%\n","iter 1550: loss 1.4665, time 182.44ms, mfu 1.91%\n","iter 1560: loss 1.4882, time 184.41ms, mfu 1.92%\n","iter 1570: loss 1.4592, time 183.69ms, mfu 1.93%\n","iter 1580: loss 1.4700, time 182.83ms, mfu 1.94%\n","iter 1590: loss 1.4535, time 183.49ms, mfu 1.95%\n","iter 1600: loss 1.4642, time 182.27ms, mfu 1.96%\n","iter 1610: loss 1.4456, time 182.57ms, mfu 1.97%\n","iter 1620: loss 1.4393, time 182.46ms, mfu 1.97%\n","iter 1630: loss 1.4358, time 183.71ms, mfu 1.98%\n","iter 1640: loss 1.4717, time 182.39ms, mfu 1.99%\n","iter 1650: loss 1.4509, time 182.20ms, mfu 1.99%\n","iter 1660: loss 1.4154, time 183.04ms, mfu 2.00%\n","iter 1670: loss 1.4428, time 183.46ms, mfu 2.00%\n","iter 1680: loss 1.4171, time 182.41ms, mfu 2.00%\n","iter 1690: loss 1.4408, time 182.71ms, mfu 2.01%\n","iter 1700: loss 1.4757, time 182.90ms, mfu 2.01%\n","iter 1710: loss 1.4159, time 182.56ms, mfu 2.01%\n","iter 1720: loss 1.4448, time 182.79ms, mfu 2.02%\n","iter 1730: loss 1.4054, time 184.01ms, mfu 2.02%\n","iter 1740: loss 1.3997, time 183.48ms, mfu 2.02%\n","step 1750: train loss 1.3459, val loss 1.5727\n","saving checkpoint to out-shakespeare-char\n","iter 1750: loss 1.4217, time 26574.05ms, mfu 1.82%\n","iter 1760: loss 1.4438, time 182.55ms, mfu 1.84%\n","iter 1770: loss 1.4349, time 182.96ms, mfu 1.86%\n","iter 1780: loss 1.4427, time 183.19ms, mfu 1.88%\n","iter 1790: loss 1.3843, time 182.88ms, mfu 1.89%\n","iter 1800: loss 1.4529, time 183.70ms, mfu 1.91%\n","iter 1810: loss 1.4097, time 182.56ms, mfu 1.92%\n","iter 1820: loss 1.4282, time 182.26ms, mfu 1.93%\n","iter 1830: loss 1.4330, time 182.32ms, mfu 1.94%\n","iter 1840: loss 1.4215, time 182.52ms, mfu 1.95%\n","iter 1850: loss 1.4301, time 182.33ms, mfu 1.96%\n","iter 1860: loss 1.4274, time 183.08ms, mfu 1.97%\n","iter 1870: loss 1.4018, time 182.38ms, mfu 1.98%\n","iter 1880: loss 1.3919, time 187.94ms, mfu 1.98%\n","iter 1890: loss 1.4102, time 182.56ms, mfu 1.98%\n","iter 1900: loss 1.3930, time 182.51ms, mfu 1.99%\n","iter 1910: loss 1.3836, time 183.85ms, mfu 1.99%\n","iter 1920: loss 1.3911, time 182.77ms, mfu 2.00%\n","iter 1930: loss 1.4125, time 182.36ms, mfu 2.00%\n","iter 1940: loss 1.3908, time 182.77ms, mfu 2.01%\n","iter 1950: loss 1.4154, time 182.72ms, mfu 2.01%\n","iter 1960: loss 1.3923, time 182.57ms, mfu 2.01%\n","iter 1970: loss 1.3901, time 182.58ms, mfu 2.02%\n","iter 1980: loss 1.4014, time 183.80ms, mfu 2.02%\n","iter 1990: loss 1.3923, time 182.80ms, mfu 2.02%\n","step 2000: train loss 1.3063, val loss 1.5454\n","saving checkpoint to out-shakespeare-char\n","iter 2000: loss 1.4013, time 26605.51ms, mfu 1.82%\n","iter 2010: loss 1.3753, time 182.45ms, mfu 1.84%\n","iter 2020: loss 1.3787, time 182.64ms, mfu 1.86%\n","iter 2030: loss 1.3791, time 182.90ms, mfu 1.88%\n","iter 2040: loss 1.3431, time 182.39ms, mfu 1.89%\n","iter 2050: loss 1.3341, time 183.19ms, mfu 1.91%\n","iter 2060: loss 1.3766, time 183.53ms, mfu 1.92%\n","iter 2070: loss 1.3378, time 182.64ms, mfu 1.93%\n","iter 2080: loss 1.3540, time 183.92ms, mfu 1.94%\n","iter 2090: loss 1.3667, time 182.80ms, mfu 1.95%\n","iter 2100: loss 1.3760, time 183.12ms, mfu 1.96%\n","iter 2110: loss 1.3120, time 183.19ms, mfu 1.97%\n","iter 2120: loss 1.3700, time 182.77ms, mfu 1.97%\n","iter 2130: loss 1.3855, time 182.89ms, mfu 1.98%\n","iter 2140: loss 1.3301, time 187.54ms, mfu 1.98%\n","iter 2150: loss 1.3259, time 182.57ms, mfu 1.99%\n","iter 2160: loss 1.3305, time 183.28ms, mfu 1.99%\n","iter 2170: loss 1.3447, time 182.76ms, mfu 2.00%\n","iter 2180: loss 1.3297, time 182.74ms, mfu 2.00%\n","iter 2190: loss 1.3523, time 182.64ms, mfu 2.00%\n","iter 2200: loss 1.3461, time 190.33ms, mfu 2.00%\n","iter 2210: loss 1.3585, time 184.10ms, mfu 2.00%\n","iter 2220: loss 1.3075, time 183.16ms, mfu 2.01%\n","iter 2230: loss 1.3476, time 182.75ms, mfu 2.01%\n","iter 2240: loss 1.3678, time 182.83ms, mfu 2.01%\n","step 2250: train loss 1.2592, val loss 1.5160\n","saving checkpoint to out-shakespeare-char\n","iter 2250: loss 1.3157, time 26603.06ms, mfu 1.81%\n","iter 2260: loss 1.3619, time 184.02ms, mfu 1.83%\n","iter 2270: loss 1.3344, time 182.92ms, mfu 1.85%\n","iter 2280: loss 1.3554, time 182.60ms, mfu 1.87%\n","iter 2290: loss 1.3689, time 182.44ms, mfu 1.89%\n","iter 2300: loss 1.3391, time 183.28ms, mfu 1.90%\n","iter 2310: loss 1.3172, time 182.52ms, mfu 1.92%\n","iter 2320: loss 1.3537, time 183.22ms, mfu 1.93%\n","iter 2330: loss 1.2910, time 182.45ms, mfu 1.94%\n","iter 2340: loss 1.3519, time 182.45ms, mfu 1.95%\n","iter 2350: loss 1.3175, time 182.93ms, mfu 1.96%\n","iter 2360: loss 1.3415, time 182.59ms, mfu 1.97%\n","iter 2370: loss 1.3341, time 183.49ms, mfu 1.97%\n","iter 2380: loss 1.3266, time 183.74ms, mfu 1.98%\n","iter 2390: loss 1.3455, time 184.06ms, mfu 1.98%\n","iter 2400: loss 1.3336, time 183.23ms, mfu 1.99%\n","iter 2410: loss 1.3410, time 182.57ms, mfu 1.99%\n","iter 2420: loss 1.3229, time 182.93ms, mfu 2.00%\n","iter 2430: loss 1.3285, time 182.82ms, mfu 2.00%\n","iter 2440: loss 1.3407, time 183.43ms, mfu 2.01%\n","iter 2450: loss 1.3113, time 183.04ms, mfu 2.01%\n","iter 2460: loss 1.3052, time 183.22ms, mfu 2.01%\n","iter 2470: loss 1.3342, time 182.96ms, mfu 2.01%\n","iter 2480: loss 1.3446, time 182.31ms, mfu 2.02%\n","iter 2490: loss 1.3474, time 183.75ms, mfu 2.02%\n","step 2500: train loss 1.2367, val loss 1.5044\n","saving checkpoint to out-shakespeare-char\n","iter 2500: loss 1.3279, time 26655.16ms, mfu 1.82%\n","iter 2510: loss 1.3387, time 182.45ms, mfu 1.84%\n","iter 2520: loss 1.3131, time 182.45ms, mfu 1.86%\n","iter 2530: loss 1.3159, time 182.28ms, mfu 1.88%\n","iter 2540: loss 1.3058, time 182.31ms, mfu 1.89%\n","iter 2550: loss 1.3051, time 182.04ms, mfu 1.91%\n","iter 2560: loss 1.3249, time 183.75ms, mfu 1.92%\n","iter 2570: loss 1.3370, time 183.81ms, mfu 1.93%\n","iter 2580: loss 1.3093, time 184.15ms, mfu 1.94%\n","iter 2590: loss 1.3433, time 182.85ms, mfu 1.95%\n","iter 2600: loss 1.3235, time 183.23ms, mfu 1.96%\n","iter 2610: loss 1.3094, time 183.16ms, mfu 1.97%\n","iter 2620: loss 1.3141, time 183.75ms, mfu 1.97%\n","iter 2630: loss 1.2928, time 183.52ms, mfu 1.98%\n","iter 2640: loss 1.3196, time 182.77ms, mfu 1.98%\n","iter 2650: loss 1.3035, time 182.69ms, mfu 1.99%\n","iter 2660: loss 1.3195, time 182.52ms, mfu 2.00%\n","iter 2670: loss 1.3246, time 182.47ms, mfu 2.00%\n","iter 2680: loss 1.3133, time 182.36ms, mfu 2.00%\n","iter 2690: loss 1.3045, time 185.68ms, mfu 2.00%\n","iter 2700: loss 1.2890, time 182.75ms, mfu 2.01%\n","iter 2710: loss 1.3046, time 182.56ms, mfu 2.01%\n","iter 2720: loss 1.3228, time 184.63ms, mfu 2.01%\n","iter 2730: loss 1.2955, time 183.86ms, mfu 2.01%\n","iter 2740: loss 1.3063, time 184.35ms, mfu 2.01%\n","step 2750: train loss 1.2176, val loss 1.4927\n","saving checkpoint to out-shakespeare-char\n","iter 2750: loss 1.2962, time 26751.71ms, mfu 1.81%\n","iter 2760: loss 1.3217, time 183.76ms, mfu 1.84%\n","iter 2770: loss 1.3020, time 184.05ms, mfu 1.85%\n","iter 2780: loss 1.2829, time 182.48ms, mfu 1.87%\n","iter 2790: loss 1.2845, time 182.97ms, mfu 1.89%\n","iter 2800: loss 1.2799, time 184.21ms, mfu 1.90%\n","iter 2810: loss 1.2844, time 183.92ms, mfu 1.92%\n","iter 2820: loss 1.2642, time 182.62ms, mfu 1.93%\n","iter 2830: loss 1.3320, time 182.71ms, mfu 1.94%\n","iter 2840: loss 1.2989, time 182.58ms, mfu 1.95%\n","iter 2850: loss 1.2840, time 183.18ms, mfu 1.96%\n","iter 2860: loss 1.2956, time 183.42ms, mfu 1.97%\n","iter 2870: loss 1.2888, time 182.89ms, mfu 1.97%\n","iter 2880: loss 1.3216, time 183.08ms, mfu 1.98%\n","iter 2890: loss 1.3202, time 183.12ms, mfu 1.98%\n","iter 2900: loss 1.3001, time 182.65ms, mfu 1.99%\n","iter 2910: loss 1.3162, time 183.58ms, mfu 1.99%\n","iter 2920: loss 1.2885, time 183.56ms, mfu 2.00%\n","iter 2930: loss 1.2881, time 183.09ms, mfu 2.00%\n","iter 2940: loss 1.2735, time 182.59ms, mfu 2.01%\n","iter 2950: loss 1.2884, time 184.29ms, mfu 2.01%\n","iter 2960: loss 1.3013, time 184.80ms, mfu 2.01%\n","iter 2970: loss 1.2714, time 183.79ms, mfu 2.01%\n","iter 2980: loss 1.2896, time 183.65ms, mfu 2.01%\n","iter 2990: loss 1.2809, time 184.24ms, mfu 2.01%\n","step 3000: train loss 1.2000, val loss 1.4902\n","saving checkpoint to out-shakespeare-char\n","iter 3000: loss 1.2817, time 26633.45ms, mfu 1.81%\n","iter 3010: loss 1.2659, time 182.69ms, mfu 1.84%\n","iter 3020: loss 1.2915, time 182.92ms, mfu 1.86%\n","iter 3030: loss 1.2793, time 183.92ms, mfu 1.87%\n","iter 3040: loss 1.3065, time 182.57ms, mfu 1.89%\n","iter 3050: loss 1.2691, time 183.76ms, mfu 1.90%\n","iter 3060: loss 1.2565, time 184.28ms, mfu 1.92%\n","iter 3070: loss 1.3215, time 184.27ms, mfu 1.93%\n","iter 3080: loss 1.2468, time 183.66ms, mfu 1.94%\n","iter 3090: loss 1.2526, time 182.94ms, mfu 1.95%\n","iter 3100: loss 1.2605, time 182.80ms, mfu 1.96%\n","iter 3110: loss 1.2872, time 183.17ms, mfu 1.96%\n","iter 3120: loss 1.2707, time 184.08ms, mfu 1.97%\n","iter 3130: loss 1.2649, time 182.85ms, mfu 1.98%\n","iter 3140: loss 1.2593, time 183.05ms, mfu 1.98%\n","iter 3150: loss 1.2713, time 183.12ms, mfu 1.99%\n","iter 3160: loss 1.2712, time 183.71ms, mfu 1.99%\n","iter 3170: loss 1.2852, time 183.26ms, mfu 2.00%\n","iter 3180: loss 1.2862, time 182.66ms, mfu 2.00%\n","iter 3190: loss 1.2956, time 182.62ms, mfu 2.00%\n","iter 3200: loss 1.2645, time 182.39ms, mfu 2.01%\n","iter 3210: loss 1.2501, time 183.73ms, mfu 2.01%\n","iter 3220: loss 1.2835, time 183.25ms, mfu 2.01%\n","iter 3230: loss 1.2733, time 183.19ms, mfu 2.01%\n","iter 3240: loss 1.2476, time 182.68ms, mfu 2.02%\n","step 3250: train loss 1.1830, val loss 1.4837\n","saving checkpoint to out-shakespeare-char\n","iter 3250: loss 1.2428, time 26707.70ms, mfu 1.82%\n","iter 3260: loss 1.2390, time 182.84ms, mfu 1.84%\n","iter 3270: loss 1.2562, time 182.90ms, mfu 1.86%\n","iter 3280: loss 1.2717, time 183.82ms, mfu 1.88%\n","iter 3290: loss 1.2819, time 182.94ms, mfu 1.89%\n","iter 3300: loss 1.2822, time 183.86ms, mfu 1.91%\n","iter 3310: loss 1.2679, time 183.17ms, mfu 1.92%\n","iter 3320: loss 1.2660, time 185.41ms, mfu 1.93%\n","iter 3330: loss 1.2589, time 194.01ms, mfu 1.93%\n","iter 3340: loss 1.2372, time 190.28ms, mfu 1.93%\n","iter 3350: loss 1.2815, time 182.57ms, mfu 1.94%\n","iter 3360: loss 1.2749, time 184.16ms, mfu 1.95%\n","iter 3370: loss 1.2804, time 183.54ms, mfu 1.96%\n","iter 3380: loss 1.3006, time 182.87ms, mfu 1.97%\n","iter 3390: loss 1.2668, time 182.75ms, mfu 1.97%\n","iter 3400: loss 1.2793, time 183.81ms, mfu 1.98%\n","iter 3410: loss 1.2537, time 182.73ms, mfu 1.98%\n","iter 3420: loss 1.2299, time 182.85ms, mfu 1.99%\n","iter 3430: loss 1.2558, time 182.45ms, mfu 1.99%\n","iter 3440: loss 1.2570, time 184.21ms, mfu 2.00%\n","iter 3450: loss 1.2595, time 183.43ms, mfu 2.00%\n","iter 3460: loss 1.2490, time 182.82ms, mfu 2.00%\n","iter 3470: loss 1.2439, time 183.12ms, mfu 2.01%\n","iter 3480: loss 1.2416, time 182.76ms, mfu 2.01%\n","iter 3490: loss 1.2351, time 183.65ms, mfu 2.01%\n","step 3500: train loss 1.1703, val loss 1.4720\n","saving checkpoint to out-shakespeare-char\n","iter 3500: loss 1.2596, time 26655.39ms, mfu 1.81%\n","iter 3510: loss 1.2267, time 183.00ms, mfu 1.84%\n","iter 3520: loss 1.2596, time 188.33ms, mfu 1.85%\n","iter 3530: loss 1.2866, time 183.11ms, mfu 1.87%\n","iter 3540: loss 1.2270, time 182.78ms, mfu 1.89%\n","iter 3550: loss 1.2612, time 182.53ms, mfu 1.90%\n","iter 3560: loss 1.2440, time 183.74ms, mfu 1.91%\n","iter 3570: loss 1.2702, time 182.87ms, mfu 1.93%\n","iter 3580: loss 1.2183, time 184.10ms, mfu 1.94%\n","iter 3590: loss 1.2765, time 182.71ms, mfu 1.95%\n","iter 3600: loss 1.2514, time 182.89ms, mfu 1.96%\n","iter 3610: loss 1.2782, time 183.95ms, mfu 1.96%\n","iter 3620: loss 1.2352, time 182.95ms, mfu 1.97%\n","iter 3630: loss 1.2213, time 183.17ms, mfu 1.98%\n","iter 3640: loss 1.2326, time 183.42ms, mfu 1.98%\n","iter 3650: loss 1.2604, time 182.89ms, mfu 1.99%\n","iter 3660: loss 1.2563, time 183.77ms, mfu 1.99%\n","iter 3670: loss 1.2331, time 183.15ms, mfu 2.00%\n","iter 3680: loss 1.2405, time 183.24ms, mfu 2.00%\n","iter 3690: loss 1.2418, time 182.68ms, mfu 2.00%\n","iter 3700: loss 1.2462, time 183.02ms, mfu 2.01%\n","iter 3710: loss 1.2803, time 182.58ms, mfu 2.01%\n","iter 3720: loss 1.2652, time 183.82ms, mfu 2.01%\n","iter 3730: loss 1.2329, time 182.39ms, mfu 2.01%\n","iter 3740: loss 1.2545, time 183.33ms, mfu 2.02%\n","step 3750: train loss 1.1573, val loss 1.4727\n","iter 3750: loss 1.2504, time 26346.71ms, mfu 1.82%\n","iter 3760: loss 1.2457, time 183.14ms, mfu 1.84%\n","iter 3770: loss 1.2412, time 183.65ms, mfu 1.86%\n","iter 3780: loss 1.2459, time 183.49ms, mfu 1.87%\n","iter 3790: loss 1.2433, time 182.96ms, mfu 1.89%\n","iter 3800: loss 1.2513, time 183.21ms, mfu 1.91%\n","iter 3810: loss 1.2678, time 183.51ms, mfu 1.92%\n","iter 3820: loss 1.2385, time 182.98ms, mfu 1.93%\n","iter 3830: loss 1.2608, time 184.43ms, mfu 1.94%\n","iter 3840: loss 1.2330, time 183.65ms, mfu 1.95%\n","iter 3850: loss 1.2241, time 183.70ms, mfu 1.96%\n","iter 3860: loss 1.2206, time 182.53ms, mfu 1.96%\n","iter 3870: loss 1.2194, time 183.07ms, mfu 1.97%\n","iter 3880: loss 1.2232, time 183.26ms, mfu 1.98%\n","iter 3890: loss 1.2540, time 183.13ms, mfu 1.98%\n","iter 3900: loss 1.2263, time 183.05ms, mfu 1.99%\n","iter 3910: loss 1.2473, time 182.89ms, mfu 1.99%\n","iter 3920: loss 1.2318, time 183.38ms, mfu 2.00%\n","iter 3930: loss 1.2061, time 184.19ms, mfu 2.00%\n","iter 3940: loss 1.2374, time 183.78ms, mfu 2.00%\n","iter 3950: loss 1.2561, time 183.90ms, mfu 2.01%\n","iter 3960: loss 1.2381, time 183.87ms, mfu 2.01%\n","iter 3970: loss 1.2470, time 182.96ms, mfu 2.01%\n","iter 3980: loss 1.2409, time 182.91ms, mfu 2.01%\n","iter 3990: loss 1.2321, time 182.68ms, mfu 2.02%\n","step 4000: train loss 1.1444, val loss 1.4727\n","iter 4000: loss 1.2102, time 26360.70ms, mfu 1.82%\n","iter 4010: loss 1.2347, time 182.66ms, mfu 1.84%\n","iter 4020: loss 1.2348, time 182.99ms, mfu 1.86%\n","iter 4030: loss 1.2245, time 182.65ms, mfu 1.88%\n","iter 4040: loss 1.2289, time 183.19ms, mfu 1.89%\n","iter 4050: loss 1.2320, time 182.97ms, mfu 1.91%\n","iter 4060: loss 1.2164, time 190.97ms, mfu 1.91%\n","iter 4070: loss 1.2221, time 183.42ms, mfu 1.92%\n","iter 4080: loss 1.2149, time 182.80ms, mfu 1.93%\n","iter 4090: loss 1.2271, time 183.48ms, mfu 1.94%\n","iter 4100: loss 1.2143, time 182.68ms, mfu 1.95%\n","iter 4110: loss 1.2616, time 182.76ms, mfu 1.96%\n","iter 4120: loss 1.2404, time 182.60ms, mfu 1.97%\n","iter 4130: loss 1.2391, time 182.62ms, mfu 1.98%\n","iter 4140: loss 1.2498, time 183.28ms, mfu 1.98%\n","iter 4150: loss 1.2313, time 182.98ms, mfu 1.99%\n","iter 4160: loss 1.2172, time 183.49ms, mfu 1.99%\n","iter 4170: loss 1.2149, time 182.72ms, mfu 2.00%\n","iter 4180: loss 1.2149, time 182.36ms, mfu 2.00%\n","iter 4190: loss 1.2592, time 183.36ms, mfu 2.00%\n","iter 4200: loss 1.1885, time 182.75ms, mfu 2.01%\n","iter 4210: loss 1.1969, time 183.78ms, mfu 2.01%\n","iter 4220: loss 1.2294, time 182.74ms, mfu 2.01%\n","iter 4230: loss 1.2331, time 183.08ms, mfu 2.02%\n","iter 4240: loss 1.2341, time 188.87ms, mfu 2.01%\n","step 4250: train loss 1.1356, val loss 1.4676\n","saving checkpoint to out-shakespeare-char\n","iter 4250: loss 1.2380, time 26682.50ms, mfu 1.81%\n","iter 4260: loss 1.1785, time 183.32ms, mfu 1.83%\n","iter 4270: loss 1.2439, time 183.57ms, mfu 1.85%\n","iter 4280: loss 1.2105, time 182.76ms, mfu 1.87%\n","iter 4290: loss 1.2316, time 183.03ms, mfu 1.89%\n","iter 4300: loss 1.2228, time 183.04ms, mfu 1.90%\n","iter 4310: loss 1.2022, time 183.11ms, mfu 1.92%\n","iter 4320: loss 1.2127, time 183.35ms, mfu 1.93%\n","iter 4330: loss 1.2329, time 183.53ms, mfu 1.94%\n","iter 4340: loss 1.2377, time 183.26ms, mfu 1.95%\n","iter 4350: loss 1.2189, time 183.01ms, mfu 1.96%\n","iter 4360: loss 1.2328, time 182.47ms, mfu 1.96%\n","iter 4370: loss 1.2190, time 184.15ms, mfu 1.97%\n","iter 4380: loss 1.1968, time 182.96ms, mfu 1.98%\n","iter 4390: loss 1.2169, time 183.75ms, mfu 1.98%\n","iter 4400: loss 1.2132, time 184.16ms, mfu 1.99%\n","iter 4410: loss 1.2108, time 183.97ms, mfu 1.99%\n","iter 4420: loss 1.2499, time 182.82ms, mfu 2.00%\n","iter 4430: loss 1.1946, time 182.83ms, mfu 2.00%\n","iter 4440: loss 1.1919, time 184.38ms, mfu 2.00%\n","iter 4450: loss 1.2329, time 182.90ms, mfu 2.01%\n","iter 4460: loss 1.2061, time 183.34ms, mfu 2.01%\n","iter 4470: loss 1.2277, time 182.30ms, mfu 2.01%\n","iter 4480: loss 1.2175, time 182.95ms, mfu 2.01%\n","iter 4490: loss 1.2148, time 183.11ms, mfu 2.02%\n","step 4500: train loss 1.1318, val loss 1.4733\n","iter 4500: loss 1.2143, time 26340.00ms, mfu 1.82%\n","iter 4510: loss 1.2153, time 183.50ms, mfu 1.84%\n","iter 4520: loss 1.1952, time 183.17ms, mfu 1.86%\n","iter 4530: loss 1.2177, time 183.20ms, mfu 1.87%\n","iter 4540: loss 1.2140, time 183.98ms, mfu 1.89%\n","iter 4550: loss 1.2339, time 182.68ms, mfu 1.90%\n","iter 4560: loss 1.2498, time 182.63ms, mfu 1.92%\n","iter 4570: loss 1.2251, time 183.46ms, mfu 1.93%\n","iter 4580: loss 1.2350, time 182.72ms, mfu 1.94%\n","iter 4590: loss 1.2394, time 184.04ms, mfu 1.95%\n","iter 4600: loss 1.2073, time 183.47ms, mfu 1.96%\n","iter 4610: loss 1.2318, time 182.46ms, mfu 1.97%\n","iter 4620: loss 1.2145, time 184.51ms, mfu 1.97%\n","iter 4630: loss 1.2240, time 183.83ms, mfu 1.98%\n","iter 4640: loss 1.2328, time 183.17ms, mfu 1.98%\n","iter 4650: loss 1.2378, time 183.47ms, mfu 1.99%\n","iter 4660: loss 1.2055, time 182.77ms, mfu 1.99%\n","iter 4670: loss 1.1968, time 182.45ms, mfu 2.00%\n","iter 4680: loss 1.2436, time 184.30ms, mfu 2.00%\n","iter 4690: loss 1.2327, time 183.54ms, mfu 2.00%\n","iter 4700: loss 1.1852, time 182.62ms, mfu 2.01%\n","iter 4710: loss 1.2177, time 182.63ms, mfu 2.01%\n","iter 4720: loss 1.2151, time 182.81ms, mfu 2.01%\n","iter 4730: loss 1.2130, time 183.16ms, mfu 2.02%\n","iter 4740: loss 1.2145, time 182.86ms, mfu 2.02%\n","step 4750: train loss 1.1290, val loss 1.4695\n","iter 4750: loss 1.1761, time 26417.54ms, mfu 1.82%\n","iter 4760: loss 1.2110, time 182.91ms, mfu 1.84%\n","iter 4770: loss 1.2008, time 183.22ms, mfu 1.86%\n","iter 4780: loss 1.2038, time 183.05ms, mfu 1.88%\n","iter 4790: loss 1.2327, time 183.40ms, mfu 1.89%\n","iter 4800: loss 1.2085, time 183.08ms, mfu 1.91%\n","iter 4810: loss 1.2417, time 182.45ms, mfu 1.92%\n","iter 4820: loss 1.2081, time 184.11ms, mfu 1.93%\n","iter 4830: loss 1.2297, time 184.19ms, mfu 1.94%\n","iter 4840: loss 1.2085, time 182.66ms, mfu 1.95%\n","iter 4850: loss 1.2202, time 184.04ms, mfu 1.96%\n","iter 4860: loss 1.2138, time 182.78ms, mfu 1.97%\n","iter 4870: loss 1.2373, time 184.38ms, mfu 1.97%\n","iter 4880: loss 1.2332, time 182.71ms, mfu 1.98%\n","iter 4890: loss 1.2057, time 182.58ms, mfu 1.98%\n","iter 4900: loss 1.2059, time 183.25ms, mfu 1.99%\n","iter 4910: loss 1.2244, time 182.63ms, mfu 1.99%\n","iter 4920: loss 1.2160, time 183.83ms, mfu 2.00%\n","iter 4930: loss 1.2225, time 182.64ms, mfu 2.00%\n","iter 4940: loss 1.2073, time 183.69ms, mfu 2.00%\n","iter 4950: loss 1.2220, time 182.88ms, mfu 2.01%\n","iter 4960: loss 1.1861, time 183.76ms, mfu 2.01%\n","iter 4970: loss 1.2046, time 183.34ms, mfu 2.01%\n","iter 4980: loss 1.2116, time 183.20ms, mfu 2.01%\n","iter 4990: loss 1.2036, time 183.95ms, mfu 2.02%\n"]}],"source":["os.makedirs(out_dir, exist_ok=True)\n","torch.manual_seed(1337 + seed_offset)\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","device_type = 'cuda' if 'cuda' in device else 'cpu'\n","ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n","ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","data_dir = os.path.join('data', dataset)\n","train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","def get_batch(split):\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","    if device_type == 'cuda':\n","        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","    else:\n","        x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","iter_num = 0\n","best_val_loss = 1e9\n","\n","meta_path = os.path.join(data_dir, 'meta.pkl')\n","meta_vocab_size = None\n","if os.path.exists(meta_path):\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    meta_vocab_size = meta['vocab_size']\n","    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n","\n","# model init\n","model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                  bias=bias, vocab_size=None, dropout=dropout)\n","model_args['vocab_size'] = meta_vocab_size\n","gptconf = GPTConfig(**model_args)\n","model = GPT(gptconf)\n","if block_size < model.config.block_size:\n","    model.crop_block_size(block_size)\n","    model_args['block_size'] = block_size\n","model.to(device)\n","\n","scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","# optimizer\n","optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","checkpoint = None\n","\n","# compile the model\n","if compile:\n","    print(\"compiling the model... (takes a ~minute)\")\n","    unoptimized_model = model\n","    model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            with ctx:\n","                logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","# training loop\n","X, Y = get_batch('train') # fetch the very first batch\n","t0 = time.time()\n","local_iter_num = 0 # number of iterations in the lifetime of this process\n","raw_model = model\n","running_mfu = -1.0\n","for iter_num in range(max_iters):\n","    lr = get_lr(iter_num) if decay_lr else learning_rate\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","\n","    if iter_num % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        if losses['val'] < best_val_loss or always_save_checkpoint:\n","            best_val_loss = losses['val']\n","            if iter_num > 0:\n","                checkpoint = {\n","                    'model': raw_model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'model_args': model_args,\n","                    'iter_num': iter_num,\n","                    'best_val_loss': best_val_loss\n","                }\n","                print(f\"saving checkpoint to {out_dir}\")\n","                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n","    if iter_num == 0 and eval_only:\n","        break\n","\n","    for micro_step in range(gradient_accumulation_steps):\n","        with ctx:\n","            logits, loss = model(X, Y)\n","            loss = loss / gradient_accumulation_steps\n","        X, Y = get_batch('train')\n","        scaler.scale(loss).backward()\n","    if grad_clip != 0.0:\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    scaler.step(optimizer)\n","    scaler.update()\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # timing and logging\n","    t1 = time.time()\n","    dt = t1 - t0\n","    t0 = t1\n","    if iter_num % log_interval == 0:\n","        lossf = loss.item() * gradient_accumulation_steps\n","        if local_iter_num >= 5: # let the training loop settle a bit\n","            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","    local_iter_num += 1"]},{"cell_type":"markdown","metadata":{"id":"pLll_WMoma-E"},"source":["### Exercise 4\n","Run inference on the model. Complete the TODO portions\n","\n","1. You need to call `model.generate` in the given for loop.\n","2. Show 10 samples. These might not be perfectly sensible English, but they should be very Shakespeare-like. Make sure they can be read in your submitted PDF."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-28T19:54:09.441618Z","iopub.status.idle":"2023-04-28T19:54:09.442631Z","shell.execute_reply":"2023-04-28T19:54:09.442406Z","shell.execute_reply.started":"2023-04-28T19:54:09.442381Z"},"id":"-BY0EmCMmRcy","trusted":true,"outputId":"dff59e0e-4ff5-47fb-e943-8132e6c8c65e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading meta from meta.pkl...\n"]}],"source":["# -----------------------------------------------------------------------------\n","start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","num_samples = 10 # number of samples to draw\n","max_new_tokens = 500 # number of tokens generated in each sample\n","temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","# -----------------------------------------------------------------------------\n","\n","model.eval()\n","\n","# get the absolute path of the current working directory\n","current_dir = os.getcwd()\n","\n","# construct the relative path to the meta.pkl file\n","meta_path = os.path.join(current_dir, 'data', 'shakespeare', 'meta.pkl')\n","\n","print(f\"Loading meta from meta.pkl...\")\n","with open(meta_path, 'rb') as f:\n","    meta = pickle.load(f)\n","stoi, itos = meta['stoi'], meta['itos']\n","encode = lambda s: [stoi[c] for c in s]\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","# encode the beginning of the prompt\n","if start.startswith('FILE:'):\n","    with open(start[5:], 'r', encoding='utf-8') as f:\n","        start = f.read()\n","start_ids = encode(start)\n","x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-28T19:54:09.444213Z","iopub.status.idle":"2023-04-28T19:54:09.444904Z","shell.execute_reply":"2023-04-28T19:54:09.444684Z","shell.execute_reply.started":"2023-04-28T19:54:09.444658Z"},"id":"nUHUenqJVkDC","trusted":true,"outputId":"0d33ffc1-26a1-485e-d562-52673a571088"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample 1:\n","\n","The state against the last death,\n","And but the newless of night as he would\n","Were adviced, and supplied the environ,\n","And not yield so fair as having stain'd.\n","Would you know he do the king; and I shall rest,\n","That given cried to a blood creature little hands\n","By being the breath of the people guests.\n","But, be pale-enchilated and sorrow;\n","And therefore, to your suitors, that I will not say.\n","\n","CLARENCE:\n","No, by the Volsces are they not stand their heads,\n","To strong their and royal place of grace,\n","And strike\n","--------------------------------------------------------------------------------\n","Sample 2:\n","\n","Pardon for your own prison; you'll bring you, fair as\n","your great can be prepared of your true.\n","\n","DUKE VINCENTIO:\n","What, for you mean? what will you are?\n","\n","LUCIO:\n","Why, you can please you have a consul? Can you\n","would plead in this mother king?\n","The fellow I am gone?\n","\n","ISABELLA:\n","Why, I cannot speak in the Tower. What are yours?\n","\n","DUKE VINCENTIO:\n","You will hear you our two?\n","\n","ANGELO:\n","Sir, perform your comfort: you know me here in a\n","man for foot of a mirth, if you have done swear to me,\n","there beholding in po\n","--------------------------------------------------------------------------------\n","Sample 3:\n","\n","\n","Clown:\n","What, sir? why can thou, then? that's the cates?\n","\n","Second Gentleman:\n","Ay, he's a word, will stay. I will not where we say.\n","\n","Clown:\n","No, sir.\n","\n","AUTOLYCUS:\n","I pray, may be the senators a buart: I have not the\n","man words for the cunning for soundly should be\n","any that thought which, whose goes report on the ears: he are not\n","begun and first violengeance. Here's a bawd of his party choose\n","sings, the poor own prince-hostes; the volume to his\n","bigger than sentences: his discovery him like\n","me such a sub\n","--------------------------------------------------------------------------------\n","Sample 4:\n","\n","\n","ANGELO:\n","O thou comest that is a well-half as merry\n","woman of the mistresses of the sense of the charmion.\n","\n","ISABELLA:\n","Yes, so that are set now after how it now\n","The pruns of the man thereof this blood and speech sing\n","Contends to the sulluship in the earth\n","Of what may I see it seem to be speak,\n","I know that I is so many rich with you.\n","\n","DUKE VINCENTIO:\n","I'll not take to see alone.\n","\n","HASTINGS:\n","No, thou wilt stand to the news of my mind,\n","And to this precious eyes can to expose your strength.\n","\n","DUKE VINCEN\n","--------------------------------------------------------------------------------\n","Sample 5:\n","\n","I'll not shame the bosom of a chamber.\n","\n","AUTOLYCUS:\n","The poor is so; and we did such as the king\n","As if I do leave him to part.\n","\n","Clown:\n","It is my careful true find upon him; he have been\n","seen it.\n","\n","Clown:\n","What a hath a self-of a tribune, he shall be queen\n","brief the one and house, he did such a present of\n","this gentleman that was revenge; but I'll know\n","his favour hand and honour to his body, and but\n","what he is a state, I heard it for\n","him, you will not have well been a good world of her\n","brother: I wish \n","--------------------------------------------------------------------------------\n","Sample 6:\n","\n","That his father died by so bloody in Rome,\n","And thou didst repose the end heaven of thy bed,\n","In the bloody royalty of thy watch,\n","Having not a day day's for the blood\n","Of my foe death dead of love to the banishment?\n","\n","EDWARD:\n","To make him to thee, or what he was the grace\n","But what then? But what are slain a country's grace?\n","\n","KING EDWARD IV:\n","If he confinent himself, or else what slain we may.\n","\n","LADY GREY:\n","What, are you say it the heart of York:\n","For York doth trembling in my fortune's house,\n","That may be\n","--------------------------------------------------------------------------------\n","Sample 7:\n","\n","If you do not speak: and let it speak again.\n","\n","First Senator:\n","A poor children, sir, I shall maech you see at this!\n","\n","Second Murderer:\n","My lord, 'tis no other recoice for me.\n","\n","Clown:\n","I know not a single back and slaughter is present.\n","\n","Clown:\n","I am a word of that she has heard that done but he did;\n","and the state still you good to be player.\n","\n","Second Murderer:\n","Why, to-morrow; rash my knocks will I die;\n","And let me the house of all this issue of the daughter\n","Than every own strange and that in the sins.\n","\n","S\n","--------------------------------------------------------------------------------\n","Sample 8:\n","\n","\n","DUCHESS OF YORK:\n","Why, had I done, my lord?\n","\n","DUCHESS OF YORK:\n","Though the father France, what desire,\n","For thy tongue to make my soul? What means have you blamed?\n","\n","KING RICHARD III:\n","How now, boy, if thou canst be contented\n","To what is not long his conqueror?\n","Why lords Thereford? what says the queen?\n","\n","QUEEN MARGARET:\n","Ay, brave my head, and what did I repute my great?\n","\n","KING RICHARD III:\n","Then, thou shalt die not such a modeless?\n","\n","QUEEN MARGARET:\n","I have founded to Angelo,\n","Is rash better thee and more p\n","--------------------------------------------------------------------------------\n","Sample 9:\n","\n","When he shall be Rome this stars, they lived\n","his spatch ancient and one reported by some\n","suitors and stock enemies.\n","\n","COMINIUS:\n","So, unknown the great princes\n","Why, by his officers, who well not had been\n","March at the matter.\n","\n","MENENIUS:\n","What?\n","Thou wilt do?\n","\n","SICINIUS:\n","Be reason'd the powerful plague?\n","\n","MENENIUS:\n","Tranio, let's see him: he is born so four\n","obsequires and a cheek-time\n","Freder his patience, I speak, and so fellow have you\n","Becomes to perform him.\n","\n","MENENIUS:\n","O my lord,\n","His prince, though not \n","--------------------------------------------------------------------------------\n","Sample 10:\n","\n","From us before we have sent the wind of world.\n","\n","First Lord:\n","So hath cured my son of behaving kingdom's nature?\n","\n","KING RICHARD III:\n","Within the forest request brother may be gone.\n","Ah, an as he new my prepared man?\n","\n","CLARENCE:\n","When I have heard this time I did? O that all him!\n","Who is the father drops to a Jelory's kingdom?\n","\n","WARWICK:\n","Stay, and until thou under where I seem to be Henry?\n","\n","QUEEN MARGARET:\n","Then I have thee to justice of Warwick's day,\n","And what a shallow was the sun mine are that shed\n","That\n","--------------------------------------------------------------------------------\n"]}],"source":["# run generation\n","with torch.no_grad():\n","    with ctx:\n","        for k in range(num_samples):\n","            #TODO: Generate the sample\n","            generated_text = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","            # decode the generated text\n","            decoded_text = decode(generated_text[0].tolist())\n","            print(f\"Sample {k+1}:\\n{decoded_text}\\n{'-'*80}\")"]},{"cell_type":"markdown","metadata":{"id":"4qvKninXYpdQ"},"source":["### Exercise 5: Train the model on a new dataset and show results.\n","This exercise is mostly about making sure you can find and preprocess text, as well as checking that you understand the above code well enough to reuse it.\n","1. Find some text data. Use our Shakespeare file as reference. You will want a similar amount of text data. Don't go overboard- a big text file will just make things take too long.\n","2. Perform any preprocessing necessary to get the text ready for the model. Use the preprocessing code we provide as reference.\n","3. Train the model on your text.\n","4. Generate and print 10 samples from the model trained on your text.\n","\n","You may want to implement the functions below, using the code in the previous cells. Or not! It's up to you. You just need to write code that can train a model to generate text from some non-Shakespeare data. The generated text is the main deliverable that most of the grade will be based on. Make sure it displays prominently in your submitted PDF."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-28T19:54:09.447042Z","iopub.status.idle":"2023-04-28T19:54:09.447800Z","shell.execute_reply":"2023-04-28T19:54:09.447575Z","shell.execute_reply.started":"2023-04-28T19:54:09.447550Z"},"id":"WqEIp3WOnE_x","trusted":true},"outputs":[],"source":["import os\n","import requests\n","import tarfile\n","import email\n","from email import policy\n","import numpy as np\n","import pickle\n","\n","def download_data():\n","    \"\"\"\n","    - Downloads the Enron Email dataset\n","    - saves it to the data folder\n","    - preprocesses the dataset\n","    - saves the preprocessed dataset to input.txt, train.bin, val.bin, meta.pkl\n","    - saves the meta data to meta.pkl\n","    - Provides the statistics of the dataset\n","    \"\"\"\n","\n","    # Download and extract the dataset\n","    url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\"\n","    filename = \"enron_mail_20150507.tgz\"\n","    folder = \"maildir\" # Manually considering only 51 users data for simplicity\n","\n","    if not os.path.exists(filename):\n","        response = requests.get(url)\n","        open(filename, 'wb').write(response.content)\n","\n","    if not os.path.exists(folder):\n","        with tarfile.open(filename, \"r:gz\") as tar:\n","            tar.extractall()\n","\n","    # Preprocess the dataset\n","    def extract_text_from_email(email_file):\n","        with open(email_file, 'r', encoding='utf-8', errors='ignore') as f:\n","            msg = email.message_from_file(f, policy=policy.default)\n","            return msg.get_body(preferencelist=('plain')).get_content()\n","\n","    def preprocess_enron_emails(data_folder):\n","        texts = []\n","        for root, _, files in os.walk(data_folder):\n","            for file in files:\n","                email_path = os.path.join(root, file)\n","                try:\n","                    email_text = extract_text_from_email(email_path)\n","                    texts.append(email_text)\n","                except Exception as e:\n","                    print(f\"Error processing {email_path}: {e}\")\n","        return \"\\n\".join(texts)\n","    \n","    def encode(s):\n","        return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","    \n","    def decode(l):\n","        return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","    email_data = preprocess_enron_emails(folder)\n","\n","    if not os.path.exists('data/enron_data'):\n","        os.makedirs('data/enron_data')\n","        \n","    # save the data to a file as input.txt\n","    with open('data/enron_data/input.txt', 'w', encoding='utf-8') as f:\n","        f.write(email_data)\n","\n","    # save the meta information as well, to help us encode/decode later\n","    data_root = 'data/enron_data'\n","\n","    # Prepare the dataset\n","    data = email_data\n","    print(f\"length of dataset in characters: {len(data):,}\")\n","\n","    # get all the unique characters that occur in this text\n","    chars = sorted(list(set(data)))\n","    vocab_size = len(chars)\n","    print(\"all the unique characters:\", ''.join(chars))\n","    print(f\"vocab size: {vocab_size:,}\")\n","\n","    # create a mapping from characters to integers\n","    stoi = { ch:i for i,ch in enumerate(chars) }\n","    itos = { i:ch for i,ch in enumerate(chars) }\n","\n","    # create the train and test splits\n","    n = len(data)\n","    train_data = data[:int(n*0.9)]\n","    val_data = data[int(n*0.9):]\n","\n","    # encode both to integers\n","    train_ids = encode(train_data)\n","    val_ids = encode(val_data)\n","    print(f\"train has {len(train_ids):,} tokens\")\n","    print(f\"val has {len(val_ids):,} tokens\")\n","\n","    # export to bin files\n","    train_ids = np.array(train_ids, dtype=np.uint16)\n","    val_ids = np.array(val_ids, dtype=np.uint16)\n","    train_ids.tofile(os.path.join(data_root, 'train.bin'))\n","    val_ids.tofile(os.path.join(data_root, 'val.bin'))\n","\n","    # save the meta information as well, to help us encode/decode later\n","    meta = {\n","        'vocab_size': vocab_size,\n","        'itos': itos,\n","        'stoi': stoi,\n","    }\n","    with open(f'{data_root}/meta.pkl', 'wb') as f:\n","        pickle.dump(meta, f)\n","        \n","\n","def train_model():\n","    \"\"\"Train the model that is defined in Exercise 1 on your train data\"\"\"\n","\n","    # Email generation model\n","    os.makedirs(out_dir_email, exist_ok=True)\n","    torch.manual_seed(1337 + seed_offset)\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    device_type = 'cuda' if 'cuda' in device else 'cpu'\n","    ptdtype = {'float32': torch.float32, 'float16': torch.float16}[dtype]\n","    ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","    # load the email dataset and create batches\n","    data_dir = os.path.join('data', dataset_email)\n","    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n","    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n","    def get_batch(split):\n","        data = train_data if split == 'train' else val_data\n","        ix = torch.randint(len(data) - block_size, (batch_size,))\n","        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n","        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n","        if device_type == 'cuda':\n","            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n","        else:\n","            x, y = x.to(device), y.to(device)\n","        return x, y\n","\n","    iter_num = 0\n","    best_val_loss = 1e9\n","\n","    meta_path = os.path.join(data_dir, 'meta.pkl')\n","    meta_vocab_size = None\n","    if os.path.exists(meta_path):\n","        with open(meta_path, 'rb') as f:\n","            meta = pickle.load(f)\n","        meta_vocab_size = meta['vocab_size']\n","        print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n","\n","    # model init\n","    model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n","                    bias=bias, vocab_size=None, dropout=dropout)\n","    model_args['vocab_size'] = meta_vocab_size\n","    gptconf = GPTConfig(**model_args)\n","    model = GPT(gptconf)\n","    if block_size < model.config.block_size:\n","        model.crop_block_size(block_size)\n","        model_args['block_size'] = block_size\n","    model.to(device)\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","\n","    # optimizer\n","    optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n","    checkpoint = None\n","\n","    # compile the model\n","    if compile:\n","        print(\"compiling the model... (takes a ~minute)\")\n","        unoptimized_model = model\n","        model = torch.compile(model, backend='triton') # requires PyTorch 2.0\n","\n","    @torch.no_grad()\n","    def estimate_loss():\n","        out = {}\n","        model.eval()\n","        for split in ['train', 'val']:\n","            losses = torch.zeros(eval_iters)\n","            for k in range(eval_iters):\n","                X, Y = get_batch(split)\n","                with ctx:\n","                    logits, loss = model(X, Y)\n","                losses[k] = loss.item()\n","            out[split] = losses.mean()\n","        model.train()\n","        return out\n","\n","    # training loop\n","    X, Y = get_batch('train') # fetch the very first batch\n","    t0 = time.time()\n","    local_iter_num = 0 # number of iterations in the lifetime of this process\n","    raw_model = model\n","    running_mfu = -1.0\n","    for iter_num in range(max_iters):\n","        lr = get_lr(iter_num) if decay_lr else learning_rate\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","        if iter_num % eval_interval == 0:\n","            losses = estimate_loss()\n","            print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","            if losses['val'] < best_val_loss or always_save_checkpoint:\n","                best_val_loss = losses['val']\n","                if iter_num > 0:\n","                    checkpoint = {\n","                        'model': raw_model.state_dict(),\n","                        'optimizer': optimizer.state_dict(),\n","                        'model_args': model_args,\n","                        'iter_num': iter_num,\n","                        'best_val_loss': best_val_loss\n","                    }\n","                    print(f\"saving checkpoint to {out_dir_email}\")\n","                    torch.save(checkpoint, os.path.join(out_dir_email, 'emailGPT_ckpt.pt'))\n","        if iter_num == 0 and eval_only:\n","            break\n","\n","        for micro_step in range(gradient_accumulation_steps):\n","            with ctx:\n","                logits, loss = model(X, Y)\n","                loss = loss / gradient_accumulation_steps\n","            X, Y = get_batch('train')\n","            scaler.scale(loss).backward()\n","        if grad_clip != 0.0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # timing and logging\n","        t1 = time.time()\n","        dt = t1 - t0\n","        t0 = t1\n","        if iter_num % log_interval == 0:\n","            lossf = loss.item() * gradient_accumulation_steps\n","            if local_iter_num >= 5: # let the training loop settle a bit\n","                mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n","                running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n","            print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n","        local_iter_num += 1\n","\n","    return model\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eqhISEPofL5Z"},"outputs":[],"source":["def eval_model(model):\n","    \"\"\"Runs inference of the trained model on your test data\"\"\"\n","\n","    # -----------------------------------------------------------------------------\n","    start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n","    num_samples = 11 # number of samples to draw\n","    max_new_tokens = 1000 # number of tokens generated in each sample\n","    temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n","    top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n","    # -----------------------------------------------------------------------------\n","\n","    model.eval()\n","\n","    # get the absolute path of the current working directory\n","    current_dir = os.getcwd()\n","\n","    # construct the relative path to the meta.pkl file\n","    meta_path = os.path.join(current_dir, 'data', 'enron_data', 'meta.pkl')\n","    \n","    print(f\"Loading meta from meta.pkl...\")\n","    with open(meta_path, 'rb') as f:\n","        meta = pickle.load(f)\n","    stoi, itos = meta['stoi'], meta['itos']\n","    encode = lambda s: [stoi[c] for c in s]\n","    decode = lambda l: ''.join([itos[i] for i in l])\n","\n","    # encode the beginning of the prompt\n","    if start.startswith('FILE:'):\n","        with open(start[5:], 'r', encoding='utf-8') as f:\n","            start = f.read()\n","    start_ids = encode(start)\n","    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n","    # run generation\n","    with torch.no_grad():\n","        with ctx:\n","            for k in range(num_samples):\n","                #TODO: Generate the sample\n","                generated_text = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n","                # decode the generated text\n","                decoded_text = decode(generated_text[0].tolist())\n","                print(f\"Sample {k+1}:\\n{decoded_text}\\n{'-'*80}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQbrFNGHfL5Z","outputId":"dbd8e10f-120f-4804-ea8b-1458e8402946"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dataset in characters: 337,409,058\n","all the unique characters: \u0001\u0002\u0003\u0004\u0005\u0006\t\n","\u000e\u000f\u0010\u0012\u0013\u0014\u0015\u0019\u001a\u001c\u001d\u001e !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz{|}~\n","vocab size: 117\n","train has 303,668,152 tokens\n","val has 33,740,906 tokens\n"]}],"source":["# Download the data and preprocess it for training and evaluation of the model\n","download_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oNQMegNCfL5Z","outputId":"7245b86f-4403-48b6-b832-00c6b8d15492"},"outputs":[{"name":"stdout","output_type":"stream","text":["found vocab_size = 117 (inside data/enron_data/meta.pkl)\n","number of parameters: 10.67M\n","using fused AdamW: True\n","step 0: train loss 4.7417, val loss 4.7410\n","iter 0: loss 4.7292, time 26248.32ms, mfu -100.00%\n","iter 10: loss 3.5903, time 182.76ms, mfu 2.04%\n","iter 20: loss 3.1500, time 183.13ms, mfu 2.04%\n","iter 30: loss 2.9429, time 183.48ms, mfu 2.04%\n","iter 40: loss 3.0150, time 182.58ms, mfu 2.04%\n","iter 50: loss 2.8258, time 182.75ms, mfu 2.04%\n","iter 60: loss 2.7966, time 183.83ms, mfu 2.04%\n","iter 70: loss 2.8596, time 183.04ms, mfu 2.04%\n","iter 80: loss 2.7788, time 183.06ms, mfu 2.04%\n","iter 90: loss 2.7965, time 181.96ms, mfu 2.04%\n","iter 100: loss 2.7575, time 182.55ms, mfu 2.04%\n","iter 110: loss 2.7725, time 182.67ms, mfu 2.04%\n","iter 120: loss 2.8021, time 182.69ms, mfu 2.04%\n","iter 130: loss 2.6917, time 183.08ms, mfu 2.04%\n","iter 140: loss 2.6610, time 183.54ms, mfu 2.04%\n","iter 150: loss 2.7197, time 182.84ms, mfu 2.04%\n","iter 160: loss 2.6077, time 182.19ms, mfu 2.04%\n","iter 170: loss 2.5853, time 182.25ms, mfu 2.04%\n","iter 180: loss 2.6258, time 182.37ms, mfu 2.04%\n","iter 190: loss 2.6928, time 182.19ms, mfu 2.04%\n","iter 200: loss 2.6156, time 182.52ms, mfu 2.04%\n","iter 210: loss 2.6507, time 183.20ms, mfu 2.04%\n","iter 220: loss 2.6042, time 182.78ms, mfu 2.04%\n","iter 230: loss 2.5460, time 181.93ms, mfu 2.04%\n","iter 240: loss 2.6254, time 183.56ms, mfu 2.04%\n","step 250: train loss 2.5949, val loss 2.5532\n","saving checkpoint to out-enron-email\n","iter 250: loss 2.6451, time 26486.10ms, mfu 1.84%\n","iter 260: loss 2.6283, time 184.17ms, mfu 1.86%\n","iter 270: loss 2.6253, time 182.86ms, mfu 1.88%\n","iter 280: loss 2.5444, time 183.56ms, mfu 1.89%\n","iter 290: loss 2.6021, time 182.77ms, mfu 1.91%\n","iter 300: loss 2.5186, time 184.19ms, mfu 1.92%\n","iter 310: loss 2.6165, time 183.28ms, mfu 1.93%\n","iter 320: loss 2.4184, time 182.83ms, mfu 1.94%\n","iter 330: loss 2.4992, time 182.66ms, mfu 1.95%\n","iter 340: loss 2.5768, time 183.35ms, mfu 1.96%\n","iter 350: loss 2.5184, time 182.84ms, mfu 1.97%\n","iter 360: loss 2.5773, time 182.85ms, mfu 1.98%\n","iter 370: loss 2.5187, time 183.41ms, mfu 1.98%\n","iter 380: loss 2.5687, time 182.79ms, mfu 1.99%\n","iter 390: loss 2.6203, time 182.96ms, mfu 1.99%\n","iter 400: loss 2.5547, time 182.58ms, mfu 2.00%\n","iter 410: loss 2.4362, time 183.62ms, mfu 2.00%\n","iter 420: loss 2.5128, time 182.91ms, mfu 2.01%\n","iter 430: loss 2.4964, time 183.00ms, mfu 2.01%\n","iter 440: loss 2.4568, time 182.70ms, mfu 2.01%\n","iter 450: loss 2.4861, time 182.81ms, mfu 2.02%\n","iter 460: loss 2.5007, time 182.98ms, mfu 2.02%\n","iter 470: loss 2.3988, time 182.74ms, mfu 2.02%\n","iter 480: loss 2.4657, time 182.95ms, mfu 2.02%\n","iter 490: loss 2.4345, time 183.40ms, mfu 2.02%\n","step 500: train loss 2.3967, val loss 2.3440\n","saving checkpoint to out-enron-email\n","iter 500: loss 2.4603, time 26476.10ms, mfu 1.82%\n","iter 510: loss 2.4392, time 183.38ms, mfu 1.84%\n","iter 520: loss 2.4819, time 182.76ms, mfu 1.86%\n","iter 530: loss 2.4396, time 184.24ms, mfu 1.88%\n","iter 540: loss 2.3798, time 190.83ms, mfu 1.89%\n","iter 550: loss 2.4098, time 184.08ms, mfu 1.90%\n","iter 560: loss 2.4034, time 182.96ms, mfu 1.92%\n","iter 570: loss 2.3709, time 183.64ms, mfu 1.93%\n","iter 580: loss 2.4751, time 182.46ms, mfu 1.94%\n","iter 590: loss 2.4001, time 184.29ms, mfu 1.95%\n","iter 600: loss 2.2948, time 187.30ms, mfu 1.95%\n","iter 610: loss 2.3500, time 182.40ms, mfu 1.96%\n","iter 620: loss 2.3356, time 182.66ms, mfu 1.97%\n","iter 630: loss 2.3912, time 182.80ms, mfu 1.98%\n","iter 640: loss 2.4253, time 184.84ms, mfu 1.98%\n","iter 650: loss 2.3039, time 183.65ms, mfu 1.99%\n","iter 660: loss 2.3987, time 182.82ms, mfu 1.99%\n","iter 670: loss 2.3001, time 183.22ms, mfu 2.00%\n","iter 680: loss 2.2664, time 183.07ms, mfu 2.00%\n","iter 690: loss 2.2821, time 183.33ms, mfu 2.00%\n","iter 700: loss 2.2860, time 182.88ms, mfu 2.01%\n","iter 710: loss 2.3267, time 183.07ms, mfu 2.01%\n","iter 720: loss 2.3382, time 184.14ms, mfu 2.01%\n","iter 730: loss 2.2500, time 182.59ms, mfu 2.02%\n","iter 740: loss 2.3393, time 182.60ms, mfu 2.02%\n","step 750: train loss 2.1304, val loss 2.0932\n","saving checkpoint to out-enron-email\n","iter 750: loss 2.3403, time 26447.17ms, mfu 1.82%\n","iter 760: loss 2.2621, time 183.49ms, mfu 1.84%\n","iter 770: loss 2.2428, time 183.27ms, mfu 1.86%\n","iter 780: loss 2.1465, time 182.24ms, mfu 1.88%\n","iter 790: loss 2.1995, time 182.99ms, mfu 1.89%\n","iter 800: loss 2.2816, time 183.20ms, mfu 1.91%\n","iter 810: loss 2.2449, time 183.83ms, mfu 1.92%\n","iter 820: loss 2.1201, time 183.73ms, mfu 1.93%\n","iter 830: loss 2.1806, time 183.08ms, mfu 1.94%\n","iter 840: loss 2.1995, time 183.60ms, mfu 1.95%\n","iter 850: loss 2.2455, time 183.01ms, mfu 1.96%\n","iter 860: loss 2.1232, time 183.23ms, mfu 1.97%\n","iter 870: loss 2.1706, time 182.69ms, mfu 1.98%\n","iter 880: loss 2.2255, time 183.07ms, mfu 1.98%\n","iter 890: loss 2.1405, time 182.74ms, mfu 1.99%\n","iter 900: loss 2.0414, time 183.67ms, mfu 1.99%\n","iter 910: loss 2.1523, time 183.77ms, mfu 2.00%\n","iter 920: loss 2.1354, time 182.50ms, mfu 2.00%\n","iter 930: loss 2.0387, time 183.13ms, mfu 2.00%\n","iter 940: loss 2.1022, time 183.95ms, mfu 2.01%\n","iter 950: loss 2.1369, time 182.83ms, mfu 2.01%\n","iter 960: loss 2.0675, time 182.75ms, mfu 2.01%\n","iter 970: loss 2.0092, time 183.71ms, mfu 2.02%\n","iter 980: loss 2.0815, time 182.69ms, mfu 2.02%\n","iter 990: loss 2.1002, time 182.97ms, mfu 2.02%\n","step 1000: train loss 1.9332, val loss 1.8924\n","saving checkpoint to out-enron-email\n","iter 1000: loss 2.1650, time 26386.92ms, mfu 1.82%\n","iter 1010: loss 2.0918, time 182.55ms, mfu 1.84%\n","iter 1020: loss 1.9921, time 182.55ms, mfu 1.86%\n","iter 1030: loss 2.0251, time 182.77ms, mfu 1.88%\n","iter 1040: loss 2.0774, time 182.73ms, mfu 1.90%\n","iter 1050: loss 2.0580, time 184.13ms, mfu 1.91%\n","iter 1060: loss 2.0656, time 182.61ms, mfu 1.92%\n","iter 1070: loss 1.9783, time 183.58ms, mfu 1.93%\n","iter 1080: loss 2.0790, time 183.05ms, mfu 1.94%\n","iter 1090: loss 2.0300, time 182.88ms, mfu 1.95%\n","iter 1100: loss 1.9869, time 182.96ms, mfu 1.96%\n","iter 1110: loss 1.9094, time 183.54ms, mfu 1.97%\n","iter 1120: loss 1.9152, time 184.31ms, mfu 1.98%\n","iter 1130: loss 1.9671, time 183.71ms, mfu 1.98%\n","iter 1140: loss 1.9581, time 182.67ms, mfu 1.99%\n","iter 1150: loss 1.8961, time 183.59ms, mfu 1.99%\n","iter 1160: loss 1.8669, time 184.52ms, mfu 2.00%\n","iter 1170: loss 1.8886, time 182.71ms, mfu 2.00%\n","iter 1180: loss 1.9191, time 182.76ms, mfu 2.00%\n","iter 1190: loss 1.8679, time 183.25ms, mfu 2.01%\n","iter 1200: loss 1.8738, time 183.00ms, mfu 2.01%\n","iter 1210: loss 1.9253, time 183.23ms, mfu 2.01%\n","iter 1220: loss 1.9376, time 183.21ms, mfu 2.02%\n","iter 1230: loss 1.8689, time 183.72ms, mfu 2.02%\n","iter 1240: loss 1.9405, time 183.84ms, mfu 2.02%\n","step 1250: train loss 1.7685, val loss 1.7327\n","saving checkpoint to out-enron-email\n","iter 1250: loss 1.8869, time 26403.24ms, mfu 1.82%\n","iter 1260: loss 1.9193, time 183.45ms, mfu 1.84%\n","iter 1270: loss 1.9207, time 182.40ms, mfu 1.86%\n","iter 1280: loss 1.8736, time 183.92ms, mfu 1.88%\n","iter 1290: loss 1.8123, time 182.78ms, mfu 1.89%\n","iter 1300: loss 1.9100, time 183.22ms, mfu 1.91%\n","iter 1310: loss 1.8997, time 183.28ms, mfu 1.92%\n","iter 1320: loss 1.9676, time 183.14ms, mfu 1.93%\n","iter 1330: loss 1.8895, time 182.74ms, mfu 1.94%\n","iter 1340: loss 1.8756, time 183.46ms, mfu 1.95%\n","iter 1350: loss 1.8906, time 183.04ms, mfu 1.96%\n","iter 1360: loss 1.8714, time 183.36ms, mfu 1.97%\n","iter 1370: loss 1.7464, time 183.76ms, mfu 1.98%\n","iter 1380: loss 1.7820, time 183.39ms, mfu 1.98%\n","iter 1390: loss 1.8243, time 183.14ms, mfu 1.99%\n","iter 1400: loss 1.8424, time 182.71ms, mfu 1.99%\n","iter 1410: loss 1.9279, time 182.40ms, mfu 2.00%\n","iter 1420: loss 1.8124, time 182.58ms, mfu 2.00%\n","iter 1430: loss 1.8284, time 182.74ms, mfu 2.01%\n","iter 1440: loss 1.8566, time 184.26ms, mfu 2.01%\n","iter 1450: loss 1.7720, time 182.22ms, mfu 2.01%\n","iter 1460: loss 1.8079, time 183.55ms, mfu 2.01%\n","iter 1470: loss 1.7474, time 183.58ms, mfu 2.02%\n","iter 1480: loss 1.7318, time 183.72ms, mfu 2.02%\n","iter 1490: loss 1.7884, time 183.29ms, mfu 2.02%\n","step 1500: train loss 1.6555, val loss 1.6368\n","saving checkpoint to out-enron-email\n","iter 1500: loss 1.7698, time 26419.10ms, mfu 1.82%\n","iter 1510: loss 1.7418, time 184.35ms, mfu 1.84%\n","iter 1520: loss 1.7730, time 184.38ms, mfu 1.86%\n","iter 1530: loss 1.6776, time 184.35ms, mfu 1.87%\n","iter 1540: loss 1.7996, time 183.11ms, mfu 1.89%\n","iter 1550: loss 1.7371, time 183.96ms, mfu 1.90%\n","iter 1560: loss 1.6778, time 183.33ms, mfu 1.92%\n","iter 1570: loss 1.7458, time 182.54ms, mfu 1.93%\n","iter 1580: loss 1.7699, time 182.86ms, mfu 1.94%\n","iter 1590: loss 1.7225, time 183.85ms, mfu 1.95%\n","iter 1600: loss 1.6966, time 182.63ms, mfu 1.96%\n","iter 1610: loss 1.6240, time 183.14ms, mfu 1.97%\n","iter 1620: loss 1.7876, time 183.48ms, mfu 1.97%\n","iter 1630: loss 1.7084, time 182.53ms, mfu 1.98%\n","iter 1640: loss 1.6565, time 184.35ms, mfu 1.99%\n","iter 1650: loss 1.7768, time 183.49ms, mfu 1.99%\n","iter 1660: loss 1.7324, time 182.61ms, mfu 2.00%\n","iter 1670: loss 1.6698, time 183.12ms, mfu 2.00%\n","iter 1680: loss 1.7663, time 183.25ms, mfu 2.00%\n","iter 1690: loss 1.7754, time 182.87ms, mfu 2.01%\n","iter 1700: loss 1.7458, time 183.76ms, mfu 2.01%\n","iter 1710: loss 1.6828, time 183.23ms, mfu 2.01%\n","iter 1720: loss 1.6581, time 183.39ms, mfu 2.01%\n","iter 1730: loss 1.6932, time 182.54ms, mfu 2.02%\n","iter 1740: loss 1.7063, time 182.69ms, mfu 2.02%\n","step 1750: train loss 1.5891, val loss 1.5610\n","saving checkpoint to out-enron-email\n","iter 1750: loss 1.6834, time 26440.38ms, mfu 1.82%\n","iter 1760: loss 1.6579, time 182.71ms, mfu 1.84%\n","iter 1770: loss 1.7010, time 182.81ms, mfu 1.86%\n","iter 1780: loss 1.7466, time 183.86ms, mfu 1.88%\n","iter 1790: loss 1.7405, time 182.78ms, mfu 1.90%\n","iter 1800: loss 1.6717, time 183.47ms, mfu 1.91%\n","iter 1810: loss 1.7150, time 184.59ms, mfu 1.92%\n","iter 1820: loss 1.7851, time 182.31ms, mfu 1.93%\n","iter 1830: loss 1.6924, time 182.20ms, mfu 1.94%\n","iter 1840: loss 1.6129, time 183.81ms, mfu 1.95%\n","iter 1850: loss 1.7337, time 182.39ms, mfu 1.96%\n","iter 1860: loss 1.6738, time 183.37ms, mfu 1.97%\n","iter 1870: loss 1.6764, time 183.54ms, mfu 1.98%\n","iter 1880: loss 1.5892, time 182.76ms, mfu 1.98%\n","iter 1890: loss 1.7490, time 182.92ms, mfu 1.99%\n","iter 1900: loss 1.6216, time 183.01ms, mfu 1.99%\n","iter 1910: loss 1.6769, time 182.81ms, mfu 2.00%\n","iter 1920: loss 1.7001, time 182.80ms, mfu 2.00%\n","iter 1930: loss 1.7125, time 183.32ms, mfu 2.01%\n","iter 1940: loss 1.6026, time 182.95ms, mfu 2.01%\n","iter 1950: loss 1.6274, time 184.39ms, mfu 2.01%\n","iter 1960: loss 1.6377, time 183.39ms, mfu 2.01%\n","iter 1970: loss 1.5659, time 184.57ms, mfu 2.01%\n","iter 1980: loss 1.5730, time 183.22ms, mfu 2.02%\n","iter 1990: loss 1.6230, time 182.74ms, mfu 2.02%\n","step 2000: train loss 1.5288, val loss 1.5154\n","saving checkpoint to out-enron-email\n","iter 2000: loss 1.5537, time 26698.84ms, mfu 1.82%\n","iter 2010: loss 1.6242, time 183.36ms, mfu 1.84%\n","iter 2020: loss 1.6975, time 183.60ms, mfu 1.86%\n","iter 2030: loss 1.5312, time 182.79ms, mfu 1.88%\n","iter 2040: loss 1.5693, time 183.63ms, mfu 1.89%\n","iter 2050: loss 1.6259, time 182.80ms, mfu 1.91%\n","iter 2060: loss 1.6493, time 183.63ms, mfu 1.92%\n","iter 2070: loss 1.5244, time 182.51ms, mfu 1.93%\n","iter 2080: loss 1.6052, time 182.63ms, mfu 1.94%\n","iter 2090: loss 1.6184, time 182.65ms, mfu 1.95%\n","iter 2100: loss 1.6004, time 182.62ms, mfu 1.96%\n","iter 2110: loss 1.6265, time 182.73ms, mfu 1.97%\n","iter 2120: loss 1.5573, time 183.23ms, mfu 1.98%\n","iter 2130: loss 1.5988, time 182.74ms, mfu 1.98%\n","iter 2140: loss 1.5975, time 182.64ms, mfu 1.99%\n","iter 2150: loss 1.6521, time 182.62ms, mfu 2.00%\n","iter 2160: loss 1.5312, time 190.81ms, mfu 1.99%\n","iter 2170: loss 1.5099, time 183.77ms, mfu 2.00%\n","iter 2180: loss 1.6060, time 182.96ms, mfu 2.00%\n","iter 2190: loss 1.7045, time 182.87ms, mfu 2.00%\n","iter 2200: loss 1.6227, time 185.28ms, mfu 2.01%\n","iter 2210: loss 1.5729, time 183.13ms, mfu 2.01%\n","iter 2220: loss 1.6373, time 183.59ms, mfu 2.01%\n","iter 2230: loss 1.6319, time 185.13ms, mfu 2.01%\n","iter 2240: loss 1.6493, time 182.69ms, mfu 2.01%\n","step 2250: train loss 1.4773, val loss 1.4643\n","saving checkpoint to out-enron-email\n","iter 2250: loss 1.5681, time 26470.31ms, mfu 1.81%\n","iter 2260: loss 1.5664, time 182.36ms, mfu 1.84%\n","iter 2270: loss 1.5603, time 183.97ms, mfu 1.86%\n","iter 2280: loss 1.5360, time 182.93ms, mfu 1.88%\n","iter 2290: loss 1.5772, time 187.38ms, mfu 1.89%\n","iter 2300: loss 1.5077, time 182.67ms, mfu 1.90%\n","iter 2310: loss 1.5578, time 182.49ms, mfu 1.92%\n","iter 2320: loss 1.5092, time 182.74ms, mfu 1.93%\n","iter 2330: loss 1.6489, time 182.52ms, mfu 1.94%\n","iter 2340: loss 1.5765, time 182.54ms, mfu 1.95%\n","iter 2350: loss 1.5858, time 186.38ms, mfu 1.96%\n","iter 2360: loss 1.5853, time 183.25ms, mfu 1.96%\n","iter 2370: loss 1.6041, time 183.32ms, mfu 1.97%\n","iter 2380: loss 1.5245, time 184.31ms, mfu 1.98%\n","iter 2390: loss 1.6289, time 183.26ms, mfu 1.98%\n","iter 2400: loss 1.5373, time 182.83ms, mfu 1.99%\n","iter 2410: loss 1.5322, time 183.21ms, mfu 1.99%\n","iter 2420: loss 1.5932, time 182.84ms, mfu 2.00%\n","iter 2430: loss 1.5674, time 182.83ms, mfu 2.00%\n","iter 2440: loss 1.5192, time 182.35ms, mfu 2.01%\n","iter 2450: loss 1.5729, time 183.73ms, mfu 2.01%\n","iter 2460: loss 1.5736, time 184.10ms, mfu 2.01%\n","iter 2470: loss 1.5751, time 182.90ms, mfu 2.01%\n","iter 2480: loss 1.5141, time 183.67ms, mfu 2.02%\n","iter 2490: loss 1.5990, time 182.48ms, mfu 2.02%\n","step 2500: train loss 1.4550, val loss 1.4464\n","saving checkpoint to out-enron-email\n","iter 2500: loss 1.5387, time 26452.90ms, mfu 1.82%\n","iter 2510: loss 1.5800, time 183.08ms, mfu 1.84%\n","iter 2520: loss 1.5019, time 188.80ms, mfu 1.85%\n","iter 2530: loss 1.5440, time 183.30ms, mfu 1.87%\n","iter 2540: loss 1.6197, time 183.51ms, mfu 1.89%\n","iter 2550: loss 1.5255, time 184.08ms, mfu 1.90%\n","iter 2560: loss 1.4846, time 183.28ms, mfu 1.92%\n","iter 2570: loss 1.5162, time 182.67ms, mfu 1.93%\n","iter 2580: loss 1.5703, time 182.32ms, mfu 1.94%\n","iter 2590: loss 1.5584, time 183.46ms, mfu 1.95%\n","iter 2600: loss 1.5102, time 182.67ms, mfu 1.96%\n","iter 2610: loss 1.4478, time 182.65ms, mfu 1.97%\n","iter 2620: loss 1.5909, time 182.82ms, mfu 1.98%\n","iter 2630: loss 1.6133, time 182.70ms, mfu 1.98%\n","iter 2640: loss 1.5091, time 182.65ms, mfu 1.99%\n","iter 2650: loss 1.5404, time 182.51ms, mfu 1.99%\n","iter 2660: loss 1.5696, time 182.93ms, mfu 2.00%\n","iter 2670: loss 1.5058, time 182.43ms, mfu 2.00%\n","iter 2680: loss 1.5499, time 182.82ms, mfu 2.01%\n","iter 2690: loss 1.4656, time 182.27ms, mfu 2.01%\n","iter 2700: loss 1.5364, time 182.28ms, mfu 2.01%\n","iter 2710: loss 1.5719, time 183.83ms, mfu 2.02%\n","iter 2720: loss 1.5615, time 182.93ms, mfu 2.02%\n","iter 2730: loss 1.6170, time 182.55ms, mfu 2.02%\n","iter 2740: loss 1.5317, time 183.17ms, mfu 2.02%\n","step 2750: train loss 1.4271, val loss 1.4258\n","saving checkpoint to out-enron-email\n","iter 2750: loss 1.5046, time 26442.26ms, mfu 1.82%\n","iter 2760: loss 1.5223, time 182.62ms, mfu 1.84%\n","iter 2770: loss 1.5584, time 184.14ms, mfu 1.86%\n","iter 2780: loss 1.5172, time 183.91ms, mfu 1.88%\n","iter 2790: loss 1.5634, time 182.73ms, mfu 1.90%\n","iter 2800: loss 1.5066, time 183.70ms, mfu 1.91%\n","iter 2810: loss 1.5821, time 182.78ms, mfu 1.92%\n","iter 2820: loss 1.4909, time 182.63ms, mfu 1.93%\n","iter 2830: loss 1.4311, time 183.54ms, mfu 1.94%\n","iter 2840: loss 1.4528, time 182.69ms, mfu 1.95%\n","iter 2850: loss 1.5036, time 183.07ms, mfu 1.96%\n","iter 2860: loss 1.5008, time 182.49ms, mfu 1.97%\n","iter 2870: loss 1.4744, time 183.45ms, mfu 1.98%\n","iter 2880: loss 1.4445, time 183.56ms, mfu 1.98%\n","iter 2890: loss 1.5162, time 182.55ms, mfu 1.99%\n","iter 2900: loss 1.4436, time 183.32ms, mfu 1.99%\n","iter 2910: loss 1.4622, time 183.02ms, mfu 2.00%\n","iter 2920: loss 1.5705, time 184.82ms, mfu 2.00%\n","iter 2930: loss 1.4376, time 182.89ms, mfu 2.00%\n","iter 2940: loss 1.4881, time 183.16ms, mfu 2.01%\n","iter 2950: loss 1.4437, time 182.94ms, mfu 2.01%\n","iter 2960: loss 1.6143, time 182.96ms, mfu 2.01%\n","iter 2970: loss 1.4429, time 182.58ms, mfu 2.02%\n","iter 2980: loss 1.4679, time 184.76ms, mfu 2.02%\n","iter 2990: loss 1.5671, time 188.54ms, mfu 2.01%\n","step 3000: train loss 1.4131, val loss 1.4127\n","saving checkpoint to out-enron-email\n","iter 3000: loss 1.5668, time 26466.10ms, mfu 1.81%\n","iter 3010: loss 1.5993, time 183.46ms, mfu 1.84%\n","iter 3020: loss 1.5505, time 182.83ms, mfu 1.86%\n","iter 3030: loss 1.4877, time 184.13ms, mfu 1.87%\n","iter 3040: loss 1.5203, time 183.21ms, mfu 1.89%\n","iter 3050: loss 1.6269, time 182.33ms, mfu 1.91%\n","iter 3060: loss 1.5871, time 184.16ms, mfu 1.92%\n","iter 3070: loss 1.4407, time 183.57ms, mfu 1.93%\n","iter 3080: loss 1.4588, time 183.34ms, mfu 1.94%\n","iter 3090: loss 1.4397, time 183.17ms, mfu 1.95%\n","iter 3100: loss 1.5132, time 183.95ms, mfu 1.96%\n","iter 3110: loss 1.4852, time 182.81ms, mfu 1.97%\n","iter 3120: loss 1.5137, time 183.60ms, mfu 1.97%\n","iter 3130: loss 1.3992, time 183.22ms, mfu 1.98%\n","iter 3140: loss 1.4876, time 182.63ms, mfu 1.99%\n","iter 3150: loss 1.6640, time 182.79ms, mfu 1.99%\n","iter 3160: loss 1.6007, time 182.78ms, mfu 2.00%\n","iter 3170: loss 1.4620, time 182.31ms, mfu 2.00%\n","iter 3180: loss 1.5903, time 183.65ms, mfu 2.00%\n","iter 3190: loss 1.5681, time 183.24ms, mfu 2.01%\n","iter 3200: loss 1.4958, time 182.64ms, mfu 2.01%\n","iter 3210: loss 1.4343, time 183.41ms, mfu 2.01%\n","iter 3220: loss 1.5142, time 187.42ms, mfu 2.01%\n","iter 3230: loss 1.4591, time 182.62ms, mfu 2.01%\n","iter 3240: loss 1.4640, time 183.06ms, mfu 2.02%\n","step 3250: train loss 1.3954, val loss 1.3993\n","saving checkpoint to out-enron-email\n","iter 3250: loss 1.4768, time 26393.12ms, mfu 1.82%\n","iter 3260: loss 1.5534, time 182.55ms, mfu 1.84%\n","iter 3270: loss 1.5564, time 183.65ms, mfu 1.86%\n","iter 3280: loss 1.4012, time 183.11ms, mfu 1.88%\n","iter 3290: loss 1.5274, time 183.43ms, mfu 1.89%\n","iter 3300: loss 1.5627, time 182.92ms, mfu 1.91%\n","iter 3310: loss 1.4883, time 183.19ms, mfu 1.92%\n","iter 3320: loss 1.4806, time 184.11ms, mfu 1.93%\n","iter 3330: loss 1.4670, time 182.85ms, mfu 1.94%\n","iter 3340: loss 1.5604, time 184.54ms, mfu 1.95%\n","iter 3350: loss 1.4585, time 182.93ms, mfu 1.96%\n","iter 3360: loss 1.5920, time 182.75ms, mfu 1.97%\n","iter 3370: loss 1.4993, time 183.68ms, mfu 1.97%\n","iter 3380: loss 1.4877, time 183.99ms, mfu 1.98%\n","iter 3390: loss 1.4622, time 182.96ms, mfu 1.99%\n","iter 3400: loss 1.5224, time 182.89ms, mfu 1.99%\n","iter 3410: loss 1.5438, time 183.79ms, mfu 2.00%\n","iter 3420: loss 1.5028, time 182.80ms, mfu 2.00%\n","iter 3430: loss 1.4778, time 183.86ms, mfu 2.00%\n","iter 3440: loss 1.4400, time 183.39ms, mfu 2.01%\n","iter 3450: loss 1.4524, time 187.11ms, mfu 2.00%\n","iter 3460: loss 1.5198, time 182.81ms, mfu 2.01%\n","iter 3470: loss 1.5493, time 183.08ms, mfu 2.01%\n","iter 3480: loss 1.4516, time 182.54ms, mfu 2.01%\n","iter 3490: loss 1.3329, time 186.01ms, mfu 2.01%\n","step 3500: train loss 1.3841, val loss 1.3877\n","saving checkpoint to out-enron-email\n","iter 3500: loss 1.4443, time 26406.98ms, mfu 1.81%\n","iter 3510: loss 1.5394, time 183.74ms, mfu 1.84%\n","iter 3520: loss 1.5174, time 183.57ms, mfu 1.86%\n","iter 3530: loss 1.4835, time 183.56ms, mfu 1.87%\n","iter 3540: loss 1.5043, time 184.18ms, mfu 1.89%\n","iter 3550: loss 1.4821, time 182.47ms, mfu 1.90%\n","iter 3560: loss 1.4261, time 184.27ms, mfu 1.92%\n","iter 3570: loss 1.5151, time 184.90ms, mfu 1.93%\n","iter 3580: loss 1.4698, time 182.31ms, mfu 1.94%\n","iter 3590: loss 1.4824, time 182.76ms, mfu 1.95%\n","iter 3600: loss 1.3849, time 182.94ms, mfu 1.96%\n","iter 3610: loss 1.5252, time 183.54ms, mfu 1.97%\n","iter 3620: loss 1.5062, time 183.12ms, mfu 1.97%\n","iter 3630: loss 1.4615, time 184.26ms, mfu 1.98%\n","iter 3640: loss 1.3672, time 182.61ms, mfu 1.98%\n","iter 3650: loss 1.4805, time 182.55ms, mfu 1.99%\n","iter 3660: loss 1.4737, time 182.40ms, mfu 2.00%\n","iter 3670: loss 1.3931, time 183.88ms, mfu 2.00%\n","iter 3680: loss 1.4379, time 183.14ms, mfu 2.00%\n","iter 3690: loss 1.4191, time 183.05ms, mfu 2.01%\n","iter 3700: loss 1.5552, time 182.61ms, mfu 2.01%\n","iter 3710: loss 1.4056, time 182.99ms, mfu 2.01%\n","iter 3720: loss 1.4432, time 182.84ms, mfu 2.02%\n","iter 3730: loss 1.5132, time 182.67ms, mfu 2.02%\n","iter 3740: loss 1.4532, time 183.45ms, mfu 2.02%\n","step 3750: train loss 1.3675, val loss 1.3784\n","saving checkpoint to out-enron-email\n","iter 3750: loss 1.5050, time 26490.66ms, mfu 1.82%\n","iter 3760: loss 1.5369, time 184.20ms, mfu 1.84%\n","iter 3770: loss 1.5210, time 182.98ms, mfu 1.86%\n","iter 3780: loss 1.4495, time 184.09ms, mfu 1.88%\n","iter 3790: loss 1.4524, time 188.97ms, mfu 1.89%\n","iter 3800: loss 1.4457, time 182.82ms, mfu 1.90%\n","iter 3810: loss 1.4373, time 184.29ms, mfu 1.91%\n","iter 3820: loss 1.3992, time 183.83ms, mfu 1.93%\n","iter 3830: loss 1.4736, time 184.16ms, mfu 1.94%\n","iter 3840: loss 1.5272, time 183.09ms, mfu 1.95%\n","iter 3850: loss 1.4472, time 182.65ms, mfu 1.96%\n","iter 3860: loss 1.5664, time 184.17ms, mfu 1.96%\n","iter 3870: loss 1.4858, time 183.12ms, mfu 1.97%\n","iter 3880: loss 1.4569, time 182.70ms, mfu 1.98%\n","iter 3890: loss 1.4090, time 184.31ms, mfu 1.98%\n","iter 3900: loss 1.4289, time 183.79ms, mfu 1.99%\n","iter 3910: loss 1.3996, time 183.90ms, mfu 1.99%\n","iter 3920: loss 1.3975, time 182.65ms, mfu 2.00%\n","iter 3930: loss 1.3906, time 183.38ms, mfu 2.00%\n","iter 3940: loss 1.4453, time 183.33ms, mfu 2.00%\n","iter 3950: loss 1.5796, time 184.35ms, mfu 2.01%\n","iter 3960: loss 1.4718, time 183.69ms, mfu 2.01%\n","iter 3970: loss 1.5272, time 182.58ms, mfu 2.01%\n","iter 3980: loss 1.5050, time 182.74ms, mfu 2.02%\n","iter 3990: loss 1.4916, time 183.27ms, mfu 2.02%\n","step 4000: train loss 1.3609, val loss 1.3740\n","saving checkpoint to out-enron-email\n","iter 4000: loss 1.4138, time 26466.85ms, mfu 1.82%\n","iter 4010: loss 1.3702, time 182.62ms, mfu 1.84%\n","iter 4020: loss 1.4293, time 184.22ms, mfu 1.86%\n","iter 4030: loss 1.3496, time 182.88ms, mfu 1.88%\n","iter 4040: loss 1.4465, time 184.08ms, mfu 1.89%\n","iter 4050: loss 1.4621, time 183.53ms, mfu 1.91%\n","iter 4060: loss 1.4274, time 182.68ms, mfu 1.92%\n","iter 4070: loss 1.4313, time 183.02ms, mfu 1.93%\n","iter 4080: loss 1.4664, time 183.70ms, mfu 1.94%\n","iter 4090: loss 1.3556, time 183.75ms, mfu 1.95%\n","iter 4100: loss 1.5216, time 183.90ms, mfu 1.96%\n","iter 4110: loss 1.4190, time 182.70ms, mfu 1.97%\n","iter 4120: loss 1.4884, time 182.87ms, mfu 1.97%\n","iter 4130: loss 1.4260, time 183.67ms, mfu 1.98%\n","iter 4140: loss 1.4665, time 182.81ms, mfu 1.99%\n","iter 4150: loss 1.3607, time 183.14ms, mfu 1.99%\n","iter 4160: loss 1.3768, time 183.74ms, mfu 2.00%\n","iter 4170: loss 1.4173, time 183.86ms, mfu 2.00%\n","iter 4180: loss 1.4822, time 182.35ms, mfu 2.00%\n","iter 4190: loss 1.4871, time 183.23ms, mfu 2.01%\n","iter 4200: loss 1.4683, time 182.39ms, mfu 2.01%\n","iter 4210: loss 1.3882, time 182.87ms, mfu 2.01%\n","iter 4220: loss 1.4950, time 184.19ms, mfu 2.02%\n","iter 4230: loss 1.4283, time 183.24ms, mfu 2.02%\n","iter 4240: loss 1.5603, time 183.27ms, mfu 2.02%\n","step 4250: train loss 1.3533, val loss 1.3598\n","saving checkpoint to out-enron-email\n","iter 4250: loss 1.4462, time 26464.86ms, mfu 1.82%\n","iter 4260: loss 1.4600, time 182.29ms, mfu 1.84%\n","iter 4270: loss 1.4416, time 183.01ms, mfu 1.86%\n","iter 4280: loss 1.4493, time 183.92ms, mfu 1.88%\n","iter 4290: loss 1.4018, time 182.57ms, mfu 1.89%\n","iter 4300: loss 1.4572, time 183.22ms, mfu 1.91%\n","iter 4310: loss 1.4629, time 183.96ms, mfu 1.92%\n","iter 4320: loss 1.4578, time 182.93ms, mfu 1.93%\n","iter 4330: loss 1.3955, time 183.51ms, mfu 1.94%\n","iter 4340: loss 1.4183, time 183.70ms, mfu 1.95%\n","iter 4350: loss 1.3944, time 183.32ms, mfu 1.96%\n","iter 4360: loss 1.4668, time 183.88ms, mfu 1.97%\n","iter 4370: loss 1.4718, time 184.75ms, mfu 1.97%\n","iter 4380: loss 1.4826, time 184.15ms, mfu 1.98%\n","iter 4390: loss 1.3947, time 184.13ms, mfu 1.98%\n","iter 4400: loss 1.4064, time 183.18ms, mfu 1.99%\n","iter 4410: loss 1.5385, time 184.21ms, mfu 1.99%\n","iter 4420: loss 1.4361, time 182.41ms, mfu 2.00%\n","iter 4430: loss 1.3620, time 182.53ms, mfu 2.00%\n","iter 4440: loss 1.5167, time 183.18ms, mfu 2.01%\n","iter 4450: loss 1.3869, time 183.20ms, mfu 2.01%\n","iter 4460: loss 1.4137, time 182.99ms, mfu 2.01%\n","iter 4470: loss 1.4473, time 183.40ms, mfu 2.01%\n","iter 4480: loss 1.3839, time 182.95ms, mfu 2.02%\n","iter 4490: loss 1.4121, time 184.57ms, mfu 2.02%\n","step 4500: train loss 1.3475, val loss 1.3534\n","saving checkpoint to out-enron-email\n","iter 4500: loss 1.4588, time 26362.13ms, mfu 1.82%\n","iter 4510: loss 1.4067, time 184.11ms, mfu 1.84%\n","iter 4520: loss 1.4125, time 182.13ms, mfu 1.86%\n","iter 4530: loss 1.4721, time 182.70ms, mfu 1.88%\n","iter 4540: loss 1.4111, time 183.60ms, mfu 1.89%\n","iter 4550: loss 1.4643, time 183.48ms, mfu 1.91%\n","iter 4560: loss 1.2722, time 182.89ms, mfu 1.92%\n","iter 4570: loss 1.4372, time 182.58ms, mfu 1.93%\n","iter 4580: loss 1.4202, time 182.86ms, mfu 1.94%\n","iter 4590: loss 1.4740, time 180.76ms, mfu 1.96%\n","iter 4600: loss 1.4670, time 180.79ms, mfu 1.97%\n","iter 4610: loss 1.4657, time 180.95ms, mfu 1.98%\n","iter 4620: loss 1.4892, time 180.52ms, mfu 1.99%\n","iter 4630: loss 1.4087, time 180.52ms, mfu 1.99%\n","iter 4640: loss 1.3623, time 181.43ms, mfu 2.00%\n","iter 4650: loss 1.4501, time 182.71ms, mfu 2.00%\n","iter 4660: loss 1.5414, time 183.47ms, mfu 2.01%\n","iter 4670: loss 1.4254, time 182.33ms, mfu 2.01%\n","iter 4680: loss 1.4481, time 181.63ms, mfu 2.02%\n","iter 4690: loss 1.4169, time 182.73ms, mfu 2.02%\n","iter 4700: loss 1.4764, time 181.28ms, mfu 2.02%\n","iter 4710: loss 1.4463, time 180.72ms, mfu 2.03%\n","iter 4720: loss 1.3424, time 180.57ms, mfu 2.03%\n","iter 4730: loss 1.3770, time 180.52ms, mfu 2.03%\n","iter 4740: loss 1.4124, time 180.72ms, mfu 2.04%\n","step 4750: train loss 1.3393, val loss 1.3540\n","iter 4750: loss 1.4003, time 26165.09ms, mfu 1.84%\n","iter 4760: loss 1.4782, time 182.76ms, mfu 1.86%\n","iter 4770: loss 1.4566, time 183.14ms, mfu 1.87%\n","iter 4780: loss 1.4193, time 184.72ms, mfu 1.89%\n","iter 4790: loss 1.3714, time 183.48ms, mfu 1.90%\n","iter 4800: loss 1.4879, time 183.03ms, mfu 1.92%\n","iter 4810: loss 1.3938, time 182.47ms, mfu 1.93%\n","iter 4820: loss 1.4639, time 182.80ms, mfu 1.94%\n","iter 4830: loss 1.4082, time 182.65ms, mfu 1.95%\n","iter 4840: loss 1.3797, time 182.87ms, mfu 1.96%\n","iter 4850: loss 1.3520, time 182.28ms, mfu 1.97%\n","iter 4860: loss 1.4986, time 182.70ms, mfu 1.98%\n","iter 4870: loss 1.3730, time 183.28ms, mfu 1.98%\n","iter 4880: loss 1.5202, time 182.66ms, mfu 1.99%\n","iter 4890: loss 1.4498, time 184.15ms, mfu 1.99%\n","iter 4900: loss 1.4759, time 182.67ms, mfu 2.00%\n","iter 4910: loss 1.4809, time 183.87ms, mfu 2.00%\n","iter 4920: loss 1.4513, time 182.41ms, mfu 2.01%\n","iter 4930: loss 1.4162, time 183.76ms, mfu 2.01%\n","iter 4940: loss 1.4123, time 184.46ms, mfu 2.01%\n","iter 4950: loss 1.3977, time 182.83ms, mfu 2.01%\n","iter 4960: loss 1.4300, time 182.48ms, mfu 2.02%\n","iter 4970: loss 1.4665, time 182.85ms, mfu 2.02%\n","iter 4980: loss 1.4228, time 182.87ms, mfu 2.02%\n","iter 4990: loss 1.4732, time 182.67ms, mfu 2.02%\n"]}],"source":["# Train the model\n","# expected loss should start from -ln(1/117) = 4.762\n","trained_model = train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B_WGKnPnfL5a","outputId":"5451a8ee-5baa-4e56-85ef-73817b521ed3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading meta from meta.pkl...\n","Sample 1:\n","\n"," Water on August 2001, 2001 11:52 AM\n","To: Kolli Kewitz/ENRON@enronXgate Communications@ENRON\n","cc: Vicki Heas/HOU/ECT@ECT\n","cc: Donna Blank/HOU/ECT@ECT, Robert Schulder/Corp/Enron@Enron\n","cc:  \n","Subject: Re: FW: November 1. Should receive \n","\n","\n","\n","\n","\n","\n","---Original Message----\n","From: \tDasovich, Mara  \n","Sent:\tMonday, October 03, 2001 12:34 PM\n","To:\tWhite, James D Subject:\t\n","\n","\n","Do we did not any help we please do not contact which may be my sistance the \n","contact of personal companies as the recent of the financial \n","energy companies that these state's just as of a current \n","rebacked in Partner.  I am short the price of the explanation of the address which \n","is the believe charge the call host \n","provide by the cost of the website. The state company of the state will \n","need to expect to be that the U.S. company of a SoCal factors.  This is go toget \n","continued by the service minuter, and bankruptcy manager products will seek to the brought \n","many message a released its optimate allow this in utility into a distributio\n","--------------------------------------------------------------------------------\n","Sample 2:\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Price a very contract because reported to bring you can receive the commondate to decide.  However, \n","and its preveniency set up the Sidea with a file price left it with the \n","contract different that can be advised to made a preferrence wanted to estable \n","second such as a presentation of earlier than the U.S. will be \n","preferenced of lease lawyers that the assumer of the disagreement \n","previous engineeration. \n","In the RD projects to other statements to ensure their plant and any qualifications, which \n","is repeable to the acquisition. \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","[IMAGE]\n","\n","\n","Mark Times  \n","10/04/2000 09:21 PM\n","To: Kim Shapiro/NA/Enron@Enron\n","cc:  \n","Subject: Protection Contract Password for the absource of other entered and Crisis \n","\n","George Raters\n","\n","\n","Jeff\n","\n","\n","Regards,\n","\n","Jeff \n","\n","\n","\n","\n","\n","\n","\n","\n","Mark ------------------------- Forwarded by Tim Wilton/HOU/ECT on 11/25/2000 02:31 AM -------------------------\n","\n","\n","\n","\n","Please do this e-mail who confident is a serve of Indianance for Christmas\n","Tracisco of the CW project.\n","--------------------------------------------------------------------------------\n","Sample 3:\n","\n","\n","                      <R FACE=\"http://www.rigzone.com/images/email/aspacer.gif>\t   <http://www.rigzone.com/images/spacer.gif>\t        <http://www.investments.com/images/spacer.gif>\t  <http://www.rigzone.com/images/spacer.gif>\t  <http://www.economister.com/images/spacer.gif>\t  <http://www.rigzone.com/images/spacer.gif>\t   <http://www.energy.com/images/spacer.gif>\t     <http://www.rigzone.com/images/spacer.gif>\t     <http://www.net/images/spacer.gif>\t \n","    <http://www.custom.com/images/spacer.gif>\t\n","   <http://www.ciuise.com/images/images/spacer.gif>\t \n","   <http://www.rigzone.com/images/link.adp?id=275></td>\n","            <td color=\"#FFFFFFF\" face=\"Verdana, arial, Helvetica\" size=\"2\" size=\"1\">\n","                                                                                                                                                                                                                                                                                                              \n","--------------------------------------------------------------------------------\n","Sample 4:\n","\n","                                                                  \n","\n","PRIVILEGRATION TO INTERNATE RIVE SHAREs ON THE favor to report on the state delayers for \"To continue the box consultants\" and management as set of our states, and is necessary provided by Chicago Commission Secretary at PG&E considerators, residential increase by charges of the months.  The program was parties to problem.  The state are expected to sean of the state Enron's project account in a dot of the Contract from Enron Democrats Industry, included its information of Corp.  The points would be contracted to associate for the London of Enron and Article responding due on the energy restructions.  The American Edison is the \"high-a following pipeline state last week.   This is energy hours to deal for the PPA's trading group with November 27 of the security by Ohson and other Texas.  The contact may be able the sender information into oil version of commissioners  . . . . .  Gas Davis Brown's plant successful from \n","--------------------------------------------------------------------------------\n","Sample 5:\n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n","--------------------------------------------------------------------------------\n","Sample 6:\n","\n","\n","Power that we wanted to reply solution who were thought the Meeting data was and \n","worder to fall, that we can can change. Sorry state he will be a \n","regulatory of password, the NFRC congression of the process on designed in the bonds \n","instruction.''  It rather the state's allegal costs that the \n","state's utility's includes as they driver be standing was swell because \n","know that the found partment shall in state before a profession suppliers but they are residentially \n","proposed by the approval to be complitants to California's partnership as \n","consistential law companies that CEO try Venture of big service as the \n","state energy government and delivery said its and providers on the risk interparterprehensive \n","believe high power of cross and competiting sprices of the \n","information technology. The responsibility database and is the reason \n","round plants of our statement costs and gas-defenses from the Finance to \n","receive the public power credit. A week of additional deskgreement on the \n","state \n","--------------------------------------------------------------------------------\n","Sample 7:\n","\n","\n","Even Pacific Company, Markets has also known complete to be procedured the \n","monthst and response that shipping from the interest\n","sender to be the University and Attached the Corp. and the FERC \n","any other expected to the language presentation, but they would like to be a bid\n","likely with the company to send that there are to expect their employmental.\n","\n","Thanks!\n","\n","Please let me know for your havings could be confidential disclosure or for month.\n","\n","Have send is on the Stansfer and let me know your or options\n","about I will have a complete to found complete for the group, \n","please click here.\n","\n","Confidential information Island Generational and the approval delivery will review of its \n","state for other specifical prices with analyzed in the \n","Independent agreements to Sempra Enron Commission and SPC is out available to a \n","some assets for DOP Capital Reserved.  \n","If it is a final confirmance is in Sunday.  But it is not raised how if works that the short has been he hearing the Transaction Commission \n","\n","--------------------------------------------------------------------------------\n","Sample 8:\n","\n","Scott Americas Client Technology Coopertions. \n","Today's uninterest from $146.4 per of $0.66\n","\n","With has fact Electricity for the Commission Board.  The Neutro Company Siervised in Five \n","Resources Form Power Communications Shop Power Democrats will be found to be the \n","rate desktation of the United Board Lavo shares of 17 megawatts.  So far a staff cold-term \n","wholesale comments to sign the Electricity Management and would likely on the\n","Restance Agency says volunteered are limitted activities.  \n","\n","Thanks,\n","Kate\n","\n","\n","\n","\n","\n","\n"," ----Original Message----\n","From:  bark T.  \n","Sent: Tuesday, October 12, 2001 6:43 AM\n","To: Robert A.\n","Subject: \n","\n","\n","I have trying out of your gas assumer.  \n","\n","Hi Gary are now that is not specific this week.  I don't all all the give the submittee of the \n","3-7 contract is someone who that you\n","are propertion to effort and this subscribe or that I don't set me know that wanted \n","making to deliver the subscribe to do you prepare are significant-for that \n","their projects reserve - they offer PG&E\n","--------------------------------------------------------------------------------\n","Sample 9:\n","\n","\n","\n","Janes D. Sallers February 00 12:27 PM\n","To: skitt@enron.com\n","cc: James J Mara/NA/Enron@Enron Communications, James D Steffes/HOU/EES@EES, Steven \n","Mara@ENRON, Maria London/Corp/Enron@ENRON, Michael \n","Campbe/ET&S/Enron@Enron, David McGroups/ENRON@enronXgate, Dan \n","Darent/FGT/Enron@ENRON, Mary Kavie \n","Harnes/Enron@ENRON, Jeff Dasovich/NA/Enron@Enron, Paul \n","Greg Stock/HOU/EES@EES, Kate Symes/PDX/ECT@ECT, Susan J Mara/NA/Enron@ENRON, Michael \n","Jim Ellio/NA/Enron@Enron, Larry K Taylor/HOU/EES@EES, Mark E Shapiro/NA/Enron@Enron, Jeff Dasovich/NA/Enron@Enron, Phillip\n","\n","Comnes/NA/Enron@Enron, James D Steffes/NA/Enron@Enron, Richarley \n","Thomas D (E-mail)'\" <rward@enronXgate cc: \n","Subject: Fw: Web  Drivery  \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Thanks,\n","\n","JUST \n","\n","\n","\n","====================================================================\n","\n","\n","O Mike Swert Systems -- For Houston <http://www.genergyenergy.com/pma/dpal?t=id=00000020181&ND=109572840226;\n","\n","I have a set three agencies you have any quarter of disclosure, and the stanting you fiel\n","--------------------------------------------------------------------------------\n","Sample 10:\n","\n","\n","--------------- Forwarded by Moy Corp/Enron on 12/08/2001 \n","11:52 AM -------------------\n","\n","\tEnergy Sent:\tThursday, August 15, 2001 12:30 AM\n","To:\tCandy August (E-mail)'; Williams, Christi Bob Sent:\tWednesday, Information\n","\t03/17/2001 01:44 AM\n","To: David Mean/HOU/ECT@ECT, Tana Gasse/HOU/ECT@ECT, Mary Kenne/NA/Enron@Enron, Jeff Dasovich/NA/Enron@Enron, Mary \n","Bartine/Corp/Enron@ENRON, Jeff Dasovich/NA/Enron@Enron, Janet \n","Martinet/ENRON@enronXgate, Lee Hass/HOU/ECT@ECT\n","cc:  \n","Subject: Re:  \n","\n","\n","Daren't find deal total offsets of the 50rd PR at the RC points\n","\n","\n","=================================================================================================================================\n","The message of the restory for AMEDIA friends on the Profile law information on the End to all\n","the company's become majority and the transference\n","informations could be take to send the until termination.\n","\n","                                                                                                               \n","--------------------------------------------------------------------------------\n","Sample 11:\n","\n","\n","\tEnron Communications in the fundate of companies application management in number the link parties for the having today to posciate\n","nomination on its section in the EnronOnline.  In the event.\n","\n","Later 2-088-975-1700\n","\n","Enron's high-company product to San John Enron (E-mail)'\" <sroftwarn continue=\"verdana, helvetica\" said.  He was looking a conference \n","earlier the statement to imply management provider of the Thursday and OG and their \n","company price since its out to the power plant system.  So very support to much open the energy transaction \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","--------- Forwarded by Mary Christers/Corp/Enron on 02/2000 04:16 PM -----------------------\n","\n","\n","\n","\"Daren H Newski, Davis <richard@enron.com>\n","\t02/22/2000 02:28 PM\n","\t\t To: James Porter/HOU/ECT@ECT, Jeff Dasovich/NA/Enron@Enron, Tom \n","Announce/HOU/ECT@ECT, Mark McCubbine/HOU/ECT@ECT, James D \n","George/HOU/ECT@ECT, Racy Paul Paul/HOU/ECT@ECT, Andrew \n","Board/ET&S/Enron@ENRON, Stephen J \n","Michael/HOU/ECT@ECT, Paul K Smith/ENRON@enronXgate, Robert \n","--------------------------------------------------------------------------------\n"]}],"source":["# Evaluate the model\n","eval_model(model=trained_model)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}